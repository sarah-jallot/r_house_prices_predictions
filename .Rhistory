fill = MSZoning)) + geom_boxplot(alpha=0.5, outlier.alpha=0) +geom_jitter(width=0.1) + geom_abline(h= mean(home$SalePrice), col='black')
mean(home$SalePrice)
#
library(ggplot2)
ggplot(home,
aes(x=MSZoning,
y=SalePrice,
colour=MSZoning,
group = MSZoning,
fill = MSZoning)) + geom_boxplot(alpha=0.5, outlier.alpha=0) +geom_jitter(width=0.1) + geom_abline(h= 180921, col='black')
mean(home$SalePrice)
#
library(ggplot2)
ggplot(home,
aes(x=MSZoning,
y=SalePrice,
colour=MSZoning,
group = MSZoning,
fill = MSZoning)) + geom_boxplot(alpha=0.5, outlier.alpha=0) +geom_jitter(width=0.1) + geom_abline(h= 180921200, col='black')
mean(home$SalePrice)
mod1<-lm(Width ~ Location-1 , data=jellyfish)
mod1<-lm(SalePrice ~ MSZoning-1 , data=home)
anova(mod1)
mod2<-lm(SalePrice ~ GarageCond-1 , data=home)
anova(mod2)
#
library(ggplot2)
ggplot(home,
aes(x=MSZoning,
y=SalePrice,
colour=MSZoning,
group = MSZoning,
fill = MSZoning)) + geom_boxplot(alpha=0.5, outlier.alpha=0) +geom_jitter(width=0.1)
head(home)
home$LotArea
head(home)
local = Poisson(0.77)
x <- seq(0,2.5,0.01)
d <- dgamma(x, shape=3, rate=5)
plot(x,d, type = 'l', main = 'Graph of the Gamma(3,5) distribution')
knitr::opts_chunk$set(echo = TRUE)
# 3 anova tests with a classical linear model
library(MASS)
# Forward method : problem with our reg0 that I dont find
reg0=lm(SalePrice~1, data=home_encoded)
linear_select_variables_forward = stepAIC(reg0, SalePrice~., data=home_encoded, trace=T, direction=c("forward"))
summary(linear_select_variables_forward)
install.packages(c("car", "fastDummies"))
par(mfrow=c(1,2))
for (colname in colnames(home_numerical)) {
hist(home_numerical[,colname], main = colname)
boxplot(home_numerical[,colname], main = colname)}
# Plot histogram grid
nums = unlist(lapply(home, is.numeric))
home_numerical = (home[ , nums])
home_numerical_output = home_numerical$SalePrice
home_numerical_features = home_numerical[,2:ncol(home_numerical)] # we will perform the PCA analysis on this dataset.
# Both method
linear_select_variables_both = stepAIC(reg0, SalePrice~., data=home_encoded, trace=TRUE, direction=c("both"))
summary(linear_select_variables_both)
# Backward method
linear_select_variables_backward = stepAIC(linear_reg,~., trace=TRUE, direction=c("backward") )
extractAIC(linear_select_variables_forward)
extractAIC(linear_select_variables_backward)
# 3 anova tests with a classic linear model
library(MASS)
# Forward method : problem with our reg0 that I dont find
reg0=lm(SalePrice~1, data=home_encoded)
linear_select_variables_forward = stepAIC(reg0, SalePrice~., data=home_encoded, trace=T, direction=c("forward"))
summary(linear_select_variables_forward)
# Backward method
linear_select_variables_backward = stepAIC(linear_reg,~., trace=TRUE, direction=c("backward") )
# Define a linear model using all features : we absolutely need to remove the last dummy
linear_reg=lm(SalePrice~., data=home_encoded)
summary(linear_reg)
# Backward method
linear_select_variables_backward = stepAIC(linear_reg,~., trace=TRUE, direction=c("backward") )
knitr::opts_chunk$set(echo = TRUE)
# Roofstyle
#table(home$RoofStyle)[order(table(home$RoofStyle))]
#levels(home$RoofStyle) <- c(levels(home$RoofStyle), "RS_Other")
#home$RoofStyle[home$RoofStyle %in% c("Shed","Mansard","Gambrel","Flat")] = "RS_Other"
# Condition 1
#table(home$Condition1)[order(table(home$Condition1))]
#levels(home$Condition1) <- c(levels(home$Condition1), "C_Other")
#home$Condition1[home$Condition1 %in% c("RRNe","RRNn","PosA","RRAe","PosN","RRAn","Artery")] = "C_Other"
# ExterCond
#table(home$ExterCond)[order(table(home$ExterCond))]
#levels(home$ExterCond) <- c(levels(home$ExterCond), "EC_Other")
#home$ExterCond[home$ExterCond %in% c("Po", "Ex", "Fa", "Gd")] = "EC_Other"
# OverallQual
#table(home$OverallQual)
#levels(home$OverallQual) <- c(levels(home$OverallQual), "Very_Low")
#home$OverallQual[home$OverallQual %in% c("-4.27238846180047", "-3.28279659466729", "-2.38962301582022")] = "Very_Low"
full_model = lm(log(SalePrice)~., data = home_streamlined)
knitr::opts_chunk$set(echo = TRUE)
# Loading the preprocessed dataset
home <- read.csv("train_preprocessed.csv", row.names=1)
head(home)
summary(home)
cat("There are", sum(is.na(home)), "missing values in our dataframe.")
# Getting regressor types
str(home, give.attr = FALSE)
cat_to_reaffect = c( "OverallQual", "OverallCond", "MoSold", "MSSubClass", "Fireplaces") # "YearBuilt", "YearRemodAdd", "GarageYrBlt", "MoSold", "YrSold", "MSSubClass")
for (category in cat_to_reaffect) {home[,category] = as.factor(home[,category])}
head(home)
# Extracting all numerical variables
nums = unlist(lapply(home, is.numeric))
home_numerical = (home[ , nums])
par(mfrow=c(1,2))
# A few observations on SalePrice.
hist(home_numerical$SalePrice, main = "SalePrice")
boxplot(home_numerical$SalePrice, main = "SalePrice")
par(mfrow=c(1,3))
hist(log(home_numerical[,"SalePrice"]))
boxplot(log(home_numerical[,"SalePrice"]))
qqnorm(log(home_numerical$SalePrice))
# Correlation matrix for numerical features
library(corrplot)
R = round(cor(home_numerical),2)
corrplot(R, method='ellipse')
par(mfrow=c(1,2))
# Area
hist(home_numerical$LotArea, main = "LotArea")
boxplot(home_numerical$LotArea, main = "LotArea")
# home_numerical_output = home_numerical$SalePrice
home_numerical_output = home_numerical$SalePrice
home_numerical_features = home_numerical[,2:ncol(home_numerical)] # we will perform the PCA analysis on this dataset.
pca.train = home_numerical_features
prin_comp = prcomp(pca.train, center = TRUE, scale. = TRUE)
# Compute standard deviation of each principal component
std_dev = prin_comp$sdev
# Compute variance
pr_var = std_dev^2
prop_varex = pr_var/sum(pr_var)
# Cumulative scree plot
plot(cumsum(prop_varex),
xlab = "Principal Component",
ylab = "Cumulative Proportion of Variance Explained",
type = "b")
abline(h=1, col='red')
abline(v=23, col='black', lty = 3)
par(mfrow=c(1,3))
# Getting our factors
factors = unlist(lapply(home, is.factor))
home_factors = home[,factors]
# Street
barplot(table(home_factors$Street)[order(table(home_factors$Street))], main = "Street")
# Neighbourhood
barplot(table(home_factors$Neighborhood)[order(table(home_factors$Neighborhood))], main = "Neighbourhood")
# Garage Finish
barplot(table(home_factors$GarageFinish)[order(table(home_factors$GarageFinish))], main = "GarageFinish")
# Finding the outliers of our model
outlierTest(linear_select_variables_backward)
# Loaging the library
library(glmnet)
# Splitting the data into regressors and output
x_vars <- model.matrix(log(SalePrice)~. , home_streamlined)[,-1]
col_to_remove = c("Street", "Utilities", "RoofMatl", "Condition2","Exterior1st", "Exterior2nd", "Electrical", "SaleCondition", "BsmtFinType2", "Heating","RoofStyle", "GarageCond", "ExterQual", "GarageFinish", "OpenPorchSF", "HeatingQC", "Functional")
for (name in col_to_remove){cat(which(colnames(home)==name)," ")}
full_model = lm(log(SalePrice)~., data = home_streamlined)
home_streamlined = home[-c(6 ,9 ,22, 14, 23, 24, 41, 67, 35, 38, 21, 60, 27, 56, 63, 39, 52)]
full_model = lm(log(SalePrice)~., data = home_streamlined)
summary(full_model)
knitr::opts_chunk$set(echo = TRUE)
par(mfrow=c(1,3))
# Plotting factor repartition
factors = unlist(lapply(home, is.factor))
home_factors = home[,factors]
for (feature in 1:ncol(home_factors)){
barplot(table(home_factors[feature])[order(table(home_factors[feature]))], main = colnames(home_factors)[feature])}
# Ancova between HeatingQC and Heating
mod3 = lm(SalePrice~1 + HeatingQC + Heating + HeatingQC:Heating, data = home)
anova(mod3)
# Ancova between HeatingQC and Heating
mod3 = lm(SalePrice~1 + Heating + HeatingQC + HeatingQC:Heating, data = home)
anova(mod3)
# Ancova between HeatingQC and Heating
mod3 = lm(SalePrice~1 + Heating + HeatingQC + Heating:HeatingQC, data = home)
anova(mod3)
# Validating the postulates
par(mfrow=c(3,3))
plot(full_model, which=1) # Errors are centered
plot(full_model, which=3) # Errors are homoscedastic
acf(residuals(full_model), main= "Auto-correlation plot") # Errors are non-correlated
plot(full_model, which=2) # Errors are gaussian
plot(full_model, which=5) # Cook's distance
# Making sure our data exploration didn't modify the dataset
home <- read.csv("train_preprocessed.csv", row.names=1)
# Some elementary pruning and regrouping of categories
# Regrouping neighbourhoods of less than 20 and 50 sales together
table_neighborhood= table(home$Neighborhood)
table_neighborhood[order(table_neighborhood)]
levels(home$Neighborhood) <- c(levels(home$Neighborhood), "N_Under20Sales","N_Under50Sales")
home$Neighborhood[home$Neighborhood %in% c("Blueste","NPkVill","Veenker","BrDale","Blmngtn","MeadowV")] = "N_Under20Sales"
home$Neighborhood[home$Neighborhood %in% c("StoneBr","SWISU","ClearCr","IDOTRR","Timber","NoRidge","Mitchel")] = "N_Under50Sales"
# Roofstyle
table(home$RoofStyle)[order(table(home$RoofStyle))]
levels(home$RoofStyle) <- c(levels(home$RoofStyle), "RS_Other")
home$RoofStyle[home$RoofStyle %in% c("Shed","Mansard","Gambrel","Flat")] = "RS_Other"
# Condition 1
table(home$Condition1)[order(table(home$Condition1))]
levels(home$Condition1) <- c(levels(home$Condition1), "C_Other")
home$Condition1[home$Condition1 %in% c("RRNe","RRNn","PosA","RRAe","PosN","RRAn","Artery")] = "C_Other"
# ExterCond
table(home$ExterCond)[order(table(home$ExterCond))]
levels(home$ExterCond) <- c(levels(home$ExterCond), "EC_Other")
home$ExterCond[home$ExterCond %in% c("Po", "Ex", "Fa", "Gd")] = "EC_Other"
# OverallQual
table(home$OverallQual)
levels(home$OverallQual) <- c(levels(home$OverallQual), "Very_Low")
home$OverallQual[home$OverallQual %in% c("-4.27238846180047", "-3.28279659466729", "-2.38962301582022")] = "Very_Low"
col_to_remove = c("Street", "Utilities", "RoofMatl", "Condition2","Exterior1st", "Exterior2nd", "Electrical", "SaleCondition", "BsmtFinType2", "Heating","RoofStyle", "GarageCond", "ExterQual", "GarageFinish", "OpenPorchSF", "HeatingQC", "Functional")
for (name in col_to_remove){cat(which(colnames(home)==name)," ")}
home_streamlined = home[-c(6 ,9 ,22, 14, 23, 24, 41, 67, 35, 38, 21, 60, 27, 56, 63, 39, 52)]
full_model = lm(log(SalePrice)~., data = home_streamlined)
summary(full_model)
# Validating the postulates
par(mfrow=c(3,3))
plot(full_model, which=1) # Errors are centered
plot(full_model, which=3) # Errors are homoscedastic
acf(residuals(full_model), main= "Auto-correlation plot") # Errors are non-correlated
plot(full_model, which=2) # Errors are gaussian
plot(full_model, which=5) # Cook's distance
# Ancova between CentralAir and Electrical
mod3 = lm(SalePrice~1 + HouseStyle + Electrical + HouseStyle:Electrical, data = home)
anova(mod3)
#
mod4 = lm(SalePrice~1 + OverallQual + GarageQual + OverallQual:GarageQual, data = home)
anova(mod4)
# Ancova between CentralAir and Electrical
mod3 = lm(SalePrice~1 + HouseStyle + Electrical + HouseStyle:Electrical, data = home)
anova(mod3)
#
mod4 = lm(SalePrice~1 + OverallQual + GarageQual + OverallQual:GarageQual, data = home)
anova(mod4)
mod5 = lm(SalePrice~1 + HeatingQC +  + HeatingQC:OverallQual, data = home)
anova(mod4)
mod5 = lm(SalePrice~1 + HeatingQC +  + HeatingQC:OverallQual, data = home)
anova(mod4)
mod5 = lm(SalePrice~1 + HeatingQC +  + HeatingQC:OverallQual, data = home)
anova(mod5)
mod5 = lm(SalePrice~1 + HeatingQC + OverallQual + HeatingQC:OverallQual, data = home)
anova(mod5)
mod5 = lm(SalePrice~1 + OverallQual + OverallCond + HeatingQC + HeatingQC:OverallQual +HeatingQC:OverallCond + HeatingQC:OverallCond:OverallQual, data = home)
anova(mod5)
mod6 = lm(SalePrice~1 + HouseStyle + RoofStyle + HouseStyle:RoofStyle, data = home)
anova(mod6)
mod5 = lm(SalePrice~1 + OverallQual + OverallCond + HeatingQC + HeatingQC:OverallQual +HeatingQC:OverallCond + HeatingQC:OverallCond:OverallQual, data = home)
anova(mod5)
mod5 = lm(SalePrice~1 + OverallQual + OverallCond + HeatingQC + HeatingQC:OverallQual +HeatingQC:OverallCond + HeatingQC:OverallCond:OverallQual, data = home)
anova(mod5)
mod5 = lm(SalePrice~1 + HeatingQC + Fireplaces, data = home)
anova(mod5)
mod5 = lm(SalePrice~1 + HeatingQC + Fireplaces + HeatingQC:Fireplaces, data = home)
anova(mod5)
# Reducing our number of features using the 3 selection methods seen in class (forward, backward, both)
library(MASS)
# Forward method :
linear_select_variables_forward = stepAIC(full_model, data=home_streamlined, trace=0, direction=c("forward"), verbose = FALSE)
#summary(linear_select_variables_forward)
# Backward method
linear_select_variables_backward = stepAIC(full_model,~., trace=0, direction=c("backward") )
#summary(linear_select_variables_backward)
# Both
linear_select_variables_both = stepAIC(full_model, trace=0, direction=c("both"))
#summary(linear_select_variables_both)
extractAIC(linear_select_variables_forward)
extractAIC(linear_select_variables_backward)
extractAIC(linear_select_variables_both)
# Validate (or not) the postulates
par(mfrow=c(3,3))
plot(linear_select_variables_backward, which=1) # P1
plot(linear_select_variables_backward, which=3) # P2
acf(residuals(linear_select_variables_backward), main= "Auto-correlation plot") # P3
plot(linear_select_variables_backward, which=2) # P4
plot(linear_select_variables_backward, which=5) # Cook
library(car)
influenceIndexPlot(linear_select_variables_backward)
library(car)
influenceIndexPlot(linear_select_variables_backward, vars = "Studentized")
# Finding the outliers of our model
outlierTest(linear_select_variables_backward)
# Finding the outliers of our model
outlierTest(linear_select_variables_backward)
home_streamlined(c(1299,524,633,1325,969,463,1433,411),1:ncol(home_streamlined))
home_streamlined[c(1299,524,633,1325,969,463,1433,411),1:ncol(home_streamlined)]
home_streamlined[c(1299,524,633,1325,969,463,1433,411),1:ncol(home_streamlined)]
max(home_streamlined$SalePrice)
home_streamlined[c(1299,524,633,1325,969,463,1433,411),1:ncol(home_streamlined)]
min(home_streamlined$SalePrice)
library(car)
influenceIndexPlot(linear_select_variables_backward)
# Validate (or not) the postulates
par(mfrow=c(3,3))
plot(lasso_regression, which=1) # P1
# Making sure our data exploration didn't modify the dataset
home <- read.csv("train_preprocessed.csv", row.names=1)
# Some elementary pruning and regrouping of categories
# Regrouping neighbourhoods of less than 20 and 50 sales together
table_neighborhood= table(home$Neighborhood)
table_neighborhood[order(table_neighborhood)]
levels(home$Neighborhood) <- c(levels(home$Neighborhood), "N_Under20Sales","N_Under50Sales")
home$Neighborhood[home$Neighborhood %in% c("Blueste","NPkVill","Veenker","BrDale","Blmngtn","MeadowV")] = "N_Under20Sales"
home$Neighborhood[home$Neighborhood %in% c("StoneBr","SWISU","ClearCr","IDOTRR","Timber","NoRidge","Mitchel")] = "N_Under50Sales"
# Some elementary pruning and regrouping of categories
# Regrouping neighbourhoods of less than 20 and 50 sales together
table_neighborhood= table(home$Neighborhood)
table_neighborhood[order(table_neighborhood)]
levels(home$Neighborhood) <- c(levels(home$Neighborhood), "N_Under20Sales","N_Under50Sales")
home$Neighborhood[home$Neighborhood %in% c("Blueste","NPkVill","Veenker","BrDale","Blmngtn","MeadowV")] = "N_Under20Sales"
home$Neighborhood[home$Neighborhood %in% c("StoneBr","SWISU","ClearCr","IDOTRR","Timber","NoRidge","Mitchel")] = "N_Under50Sales"
# Roofstyle
table(home$RoofStyle)[order(table(home$RoofStyle))]
levels(home$RoofStyle) <- c(levels(home$RoofStyle), "RS_Other")
home$RoofStyle[home$RoofStyle %in% c("Shed","Mansard","Gambrel","Flat")] = "RS_Other"
# Condition 1
table(home$Condition1)[order(table(home$Condition1))]
levels(home$Condition1) <- c(levels(home$Condition1), "C_Other")
home$Condition1[home$Condition1 %in% c("RRNe","RRNn","PosA","RRAe","PosN","RRAn","Artery")] = "C_Other"
# ExterCond
table(home$ExterCond)[order(table(home$ExterCond))]
levels(home$ExterCond) <- c(levels(home$ExterCond), "EC_Other")
home$ExterCond[home$ExterCond %in% c("Po", "Ex", "Fa", "Gd")] = "EC_Other"
# OverallQual
table(home$OverallQual)
levels(home$OverallQual) <- c(levels(home$OverallQual), "Very_Low")
home$OverallQual[home$OverallQual %in% c("-4.27238846180047", "-3.28279659466729", "-2.38962301582022")] = "Very_Low"
col_to_remove = c("Street", "Utilities", "RoofMatl", "Condition2","Exterior1st", "Exterior2nd", "Electrical", "SaleCondition", "BsmtFinType2", "Heating","RoofStyle", "GarageCond", "ExterQual", "GarageFinish", "OpenPorchSF", "HeatingQC", "Functional")
for (name in col_to_remove){cat(which(colnames(home)==name)," ")}
home_streamlined = home[-c(6 ,9 ,22, 14, 23, 24, 41, 67, 35, 38, 21, 60, 27, 56, 63, 39, 52)]
full_model = lm(log(SalePrice)~., data = home_streamlined)
summary(full_model)
# Validating the postulates
par(mfrow=c(3,3))
plot(full_model, which=1) # Errors are centered
plot(full_model, which=3) # Errors are homoscedastic
acf(residuals(full_model), main= "Auto-correlation plot") # Errors are non-correlated
plot(full_model, which=2) # Errors are gaussian
plot(full_model, which=5) # Cook's distance
# Reducing our number of features using the 3 selection methods seen in class (forward, backward, both)
library(MASS)
# Forward method :
linear_select_variables_forward = stepAIC(full_model, data=home_streamlined, trace=0, direction=c("forward"), verbose = FALSE)
#summary(linear_select_variables_forward)
# Backward method
linear_select_variables_backward = stepAIC(full_model,~., trace=0, direction=c("backward") )
#summary(linear_select_variables_backward)
# Both
linear_select_variables_both = stepAIC(full_model, trace=0, direction=c("both"))
#summary(linear_select_variables_both)
par(mfrow=c(1,2))
for (colname in colnames(home_numerical)) {
hist(home_numerical[,colname], main = colname)
boxplot(home_numerical[,colname], main = colname)}
col_to_remove = c("Street", "Utilities", "RoofMatl", "Condition2","Exterior1st", "Exterior2nd", "Electrical", "SaleCondition", "BsmtFinType2", "Heating","RoofStyle", "GarageCond", "ExterQual", "GarageFinish", "OpenPorchSF", "HeatingQC", "Functional")
for (name in col_to_remove){cat(which(colnames(home)==name)," ")}
home_streamlined = home[-c(6 ,9 ,22, 14, 23, 24, 41, 67, 35, 38, 21, 60, 27, 56, 63, 39, 52)]
# Some elementary pruning and regrouping of categories
# Regrouping neighbourhoods of less than 20 and 50 sales together
table_neighborhood= table(home$Neighborhood)
table_neighborhood[order(table_neighborhood)]
levels(home$Neighborhood) <- c(levels(home$Neighborhood), "N_Under20Sales","N_Under50Sales")
home$Neighborhood[home$Neighborhood %in% c("Blueste","NPkVill","Veenker","BrDale","Blmngtn","MeadowV")] = "N_Under20Sales"
home$Neighborhood[home$Neighborhood %in% c("StoneBr","SWISU","ClearCr","IDOTRR","Timber","NoRidge","Mitchel")] = "N_Under50Sales"
# Roofstyle
table(home$RoofStyle)[order(table(home$RoofStyle))]
levels(home$RoofStyle) <- c(levels(home$RoofStyle), "RS_Other")
home$RoofStyle[home$RoofStyle %in% c("Shed","Mansard","Gambrel","Flat")] = "RS_Other"
# Condition 1
table(home$Condition1)[order(table(home$Condition1))]
levels(home$Condition1) <- c(levels(home$Condition1), "C_Other")
home$Condition1[home$Condition1 %in% c("RRNe","RRNn","PosA","RRAe","PosN","RRAn","Artery")] = "C_Other"
# ExterCond
table(home$ExterCond)[order(table(home$ExterCond))]
levels(home$ExterCond) <- c(levels(home$ExterCond), "EC_Other")
home$ExterCond[home$ExterCond %in% c("Po", "Ex", "Fa", "Gd")] = "EC_Other"
# OverallQual
table(home$OverallQual)
levels(home$OverallQual) <- c(levels(home$OverallQual), "Very_Low")
home <- read.csv("train_preprocessed.csv", row.names=1)
# Some elementary pruning and regrouping of categories
# Regrouping neighbourhoods of less than 20 and 50 sales together
table_neighborhood= table(home$Neighborhood)
table_neighborhood[order(table_neighborhood)]
levels(home$Neighborhood) <- c(levels(home$Neighborhood), "N_Under20Sales","N_Under50Sales")
home$Neighborhood[home$Neighborhood %in% c("Blueste","NPkVill","Veenker","BrDale","Blmngtn","MeadowV")] = "N_Under20Sales"
home$Neighborhood[home$Neighborhood %in% c("StoneBr","SWISU","ClearCr","IDOTRR","Timber","NoRidge","Mitchel")] = "N_Under50Sales"
# Roofstyle
table(home$RoofStyle)[order(table(home$RoofStyle))]
levels(home$RoofStyle) <- c(levels(home$RoofStyle), "RS_Other")
home$RoofStyle[home$RoofStyle %in% c("Shed","Mansard","Gambrel","Flat")] = "RS_Other"
# Condition 1
table(home$Condition1)[order(table(home$Condition1))]
levels(home$Condition1) <- c(levels(home$Condition1), "C_Other")
home$Condition1[home$Condition1 %in% c("RRNe","RRNn","PosA","RRAe","PosN","RRAn","Artery")] = "C_Other"
# ExterCond
table(home$ExterCond)[order(table(home$ExterCond))]
levels(home$ExterCond) <- c(levels(home$ExterCond), "EC_Other")
home$ExterCond[home$ExterCond %in% c("Po", "Ex", "Fa", "Gd")] = "EC_Other"
# OverallQual
table(home$OverallQual)
levels(home$OverallQual) <- c(levels(home$OverallQual), "Very_Low")
home$OverallQual[home$OverallQual %in% c("-4.27238846180047", "-3.28279659466729", "-2.38962301582022")] = "Very_Low"
col_to_remove = c("Street", "Utilities", "RoofMatl", "Condition2","Exterior1st", "Exterior2nd", "Electrical", "SaleCondition", "BsmtFinType2", "Heating","RoofStyle", "GarageCond", "ExterQual", "GarageFinish", "OpenPorchSF", "HeatingQC", "Functional")
for (name in col_to_remove){cat(which(colnames(home)==name)," ")}
home_streamlined = home[-c(6 ,9 ,22, 14, 23, 24, 41, 67, 35, 38, 21, 60, 27, 56, 63, 39, 52)]
## set the seed to make your partition reproducible
smp_size = 0.8*nrow(home_streamlined)
set.seed(123)
train_ind <- sample(seq_len(nrow(home_streamlined), size = smp_size)
train <- home_streamlined[train_ind, ]
## set the seed to make your partition reproducible
smp_size = 0.8*nrow(home_streamlined)
set.seed(123)
train_ind <- sample(seq_len(nrow(home_streamlined), size = smp_size)
train <- home_streamlined[train_ind, ]
## set the seed to make your partition reproducible
smp_size = 0.8*nrow(home_streamlined)
set.seed(123)
train_ind = sample(seq_len(nrow(home_streamlined), size = smp_size)
train = home_streamlined[train_ind, ]
# Splitting the data into test and train
set.seed(86)
train = sample(1:nrow(x_vars), nrow(x_vars)/2)
# Splitting the data into test and train
set.seed(86)
x_vars <- model.matrix(log(SalePrice)~. , home_streamlined)[,-1]
y_var <- log(home_streamlined$SalePrice)
lambda_seq <- 10^seq(2, -2, by = -.1)
train = sample(1:nrow(x_vars), nrow(x_vars)/2)
x_test = (-train)
y_test = y_var[x_test]
predict(linear_select_variables_backward,
x_test,
se.fit = FALSE,
scale = NULL,
df = Inf,
interval = c("none", "confidence", "prediction"),
level = 0.95, type = c("response", "terms"),
terms = NULL, na.action = na.pass,
pred.var = res.var/weights, weights = 1, ...)
predict(linear_select_variables_backward,
x_test,
se.fit = FALSE,
scale = NULL,
df = Inf,
interval = c("none", "confidence", "prediction"),
level = 0.95, type = c("response", "terms"),
terms = NULL, na.action = na.pass,
pred.var = res.var/weights, weights = 1)
predict(linear_select_variables_backward,
x_test,
se.fit = FALSE,
scale = NULL,
df = Inf,
interval = c("none", "confidence", "prediction"),
level = 0.95, type = c("response", "terms"),
terms = NULL,
na.action = na.pass,
pred.var = res.var/weights,
weights = 1)
# home_streamlined[c(186, 251, 667),1:ncol(home_streamlined)]
train_indice = floor(0.8*nrow(home_streamlined))
training_data = home_streamlined[1:train_indice, 1:ncol(home_streamlined)]
full_model = lm(log(SalePrice)~., data = training_data)
summary(full_model)
# Validating the postulates
par(mfrow=c(3,3))
plot(full_model, which=1) # Errors are centered
plot(full_model, which=3) # Errors are homoscedastic
acf(residuals(full_model), main= "Auto-correlation plot") # Errors are non-correlated
plot(full_model, which=2) # Errors are gaussian
plot(full_model, which=5) # Cook's distance
# Reducing our number of features using the 3 selection methods seen in class (forward, backward, both)
library(MASS)
# Forward method :
linear_select_variables_forward = stepAIC(full_model, data=home_streamlined, trace=0, direction=c("forward"), verbose = FALSE)
#summary(linear_select_variables_forward)
# Backward method
linear_select_variables_backward = stepAIC(full_model,~., trace=0, direction=c("backward") )
#summary(linear_select_variables_backward)
# Both
linear_select_variables_both = stepAIC(full_model, trace=0, direction=c("both"))
#summary(linear_select_variables_both)
extractAIC(linear_select_variables_forward)
extractAIC(linear_select_variables_backward)
extractAIC(linear_select_variables_both)
# Validate (or not) the postulates
par(mfrow=c(3,3))
plot(linear_select_variables_backward, which=1) # P1
plot(linear_select_variables_backward, which=3) # P2
acf(residuals(linear_select_variables_backward), main= "Auto-correlation plot") # P3
plot(linear_select_variables_backward, which=2) # P4
plot(linear_select_variables_backward, which=5) # Cook
library(car)
influenceIndexPlot(linear_select_variables_backward)
# Finding the outliers of our model
outlierTest(linear_select_variables_backward)
# Loaging the library
library(glmnet)
# Splitting the data into regressors and output
x_vars <- model.matrix(log(SalePrice)~. , home_streamlined)[,-1]
y_var <- log(home_streamlined$SalePrice)
lambda_seq <- 10^seq(2, -2, by = -.1)
# Splitting the data into test and train
set.seed(86)
train = sample(1:nrow(x_vars), nrow(x_vars)/2)
x_test = (-train)
y_test = y_var[x_test]
cv_output <- cv.glmnet(x_vars[train,], y_var[train],
alpha = 1, lambda = lambda_seq)
