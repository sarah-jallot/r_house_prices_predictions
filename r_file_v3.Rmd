---
title: "Regression - Final project"
author: "Victoire de Termont and Sarah Jallot"
date: "12/3/2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

\textbf{I. Introduction} 
  
Our objective is to predict the prices of residential homes in Ames, Iowa accurately by exploiting an existing dataset to its best. Our key metric will be the RMSE on test data.     
The training data consists of a set of 66 categorical and quantitative variables extensively describing 1,460 residential homes in Ames, Iowa, and their corresponding price in dollars. 
We want to preprocess and explore our data smartly, select the best performing features, and fit the most adapted regression model after validating the key assumptions needed to implement it.     
  
We will first explore our data to get intuition of an efficient model. Understanding and preprocessing the data implies that we understand what we are given, check that there are no missing values, and that R correctly categorises each regressor.  
  
```{r, echo=TRUE}
# Loading the preprocessed dataset 
home <- read.csv("train_preprocessed.csv", row.names=1)
head(home)
```
The data we are handling is heterogeneous, although it is presented in a structured manner. We are dealing both with categorical and quantitative variables, which we will have to preprocess separately.  
  
`````{r, echo=TRUE}
summary(home)
```
  
In our dataframe, there are 67 columns including the price, meaning a full model would include 66 features and an intercept, and 1,460 observations.  
The list of features is extensive: it is probable that not all regressors are useful to predict house price.  
  
The quantitative variables differ in scale and range: prices start from ~35,000 dollars, and can attain 755,000, whereas bedrooms above ground range from 1 to 8 only for instance.  
This means that scaling our data could optimise model performance.  
  
We first treat missing values before launching into analysis.  
  
```{r, echo=TRUE}
cat("There are", sum(is.na(home)), "missing values in our dataframe.")
```

```{r, echo=TRUE}
# Getting regressor types
str(home, give.attr = FALSE)
```
First intuition: looking at this dataset, we can see that we have a lot of regressors and that some might be more relevant than others to explain the SalePrice. For instance, the variable MSZoning is expected to have much more impact on the price that the variable Heating, as the heating system is something that can be changed, whereas the location is permanent.  
Thus, we will perform different types of analysis to keep only the most relevant variables in our model. 

Additionally, R appears to classify some features as integers when they could be considered as factors: mainly factors related to areas and quality ratings. After debating whether we would consider Years as categories, we decided against given that we might come across a previously unencountered year in the test data which would nullify the interest of using year as a factor.
  
All quantitative features are integers and price, the output, is the only numeric.  
Factor variables are automatically numerically encoded by R, which assigns them to ascending levels using their alphabetical order. Note that this might bias the data by assigning a higher value to one level versus another without any ground to do so.  
```{r}
cat_to_reaffect = c( "OverallQual", "OverallCond", "MoSold", "MSSubClass", "Fireplaces") # "YearBuilt", "YearRemodAdd", "GarageYrBlt", "MoSold", "YrSold", "MSSubClass")
for (category in cat_to_reaffect) {home[,category] = as.factor(home[,category])}
head(home)
```
```{r}
str(home)
```

\textbf{II. Exploratory data analysis / initial modeling} 

1. Numerical analysis  
We will extract trends and relationships among the regressors. Comparing them is also essential, for instance using pairplots.
```{r, echo=TRUE}
# Extracting all numerical variables
nums = unlist(lapply(home, is.numeric))
home_numerical = (home[ , nums])
```

We first set out to observe the output, SalePrice. 
```{r, echo=TRUE}
par(mfrow=c(1,2))
# A few observations on SalePrice. 
hist(home_numerical$SalePrice, main = "SalePrice")
boxplot(home_numerical$SalePrice, main = "SalePrice")
```

```{r}
cat("The mean of Saleprice is", mean(home_numerical$SalePrice), "whereas the median is higher at", median(home_numerical$SalePrice), "indicating that extreme values skew our data to the right.")
```
SalePrice is volatile with many houses to the left hand side, but with an important number of outliers to the right with extreme values which skew the data. To smoothen the output and approach a normal distribution, we will consider the log when fitting our model. 
```{r}
par(mfrow=c(1,3))
hist(log(home_numerical[,"SalePrice"]))
boxplot(log(home_numerical[,"SalePrice"]))
qqnorm(log(home_numerical$SalePrice))
```
log(SalePrice) is pretty close to a normal distribution, except for extreme values. 

```{r}
# Correlation matrix for numerical features
library(corrplot)
R = round(cor(home_numerical),2)
corrplot(R, method='ellipse')
```
Our intuition is that three feature types mainly drive SalePrices: location, area/surface and quality. Let's start by analysing the numerical variable LotArea.  
```{r, echo=TRUE}
par(mfrow=c(1,2))
# Area
hist(home_numerical$LotArea, main = "LotArea")
boxplot(home_numerical$LotArea, main = "LotArea")
```
We believe that areas, particularly LotArea, will be important when predicting SalePrice. 
Most areas are located within [-2,2] in our pre-processed dataset, with many outliers towards the left- or the right-hand-side. 

Looking at inter-feature correlations first, we observe that OverallQual is correlated to may other features on the correlation matrix. This indicates that there is a link between it and other variables. This is encouraging as it means that we would be able to remove some columns without losing too much information.

The numerical features that are the most correlated with SalePrice are OverallQual, TotalBsmtSE, X1stElrSF, GrLivArea, GarageCars and GarageArea. On the contrary, MoSold, YrSold and BsmtHalfBath are poorly correlated to SalePrice. As the correlation matrix does not take into account interactions between our features in predicting sales price, we will not infer from this which variables we will keep in our final model.  

## PCA 
Given that we have 67 columns, of which 29 are quantitative, let's explore the possibilities we have to reduce our dataframe.  
Because PCA works best with (scaled) numerical data, we will perform it on our scaled numerical feature columns. 
```{r, echo=TRUE}
# home_numerical_output = home_numerical$SalePrice
home_numerical_output = home_numerical$SalePrice
home_numerical_features = home_numerical[,2:ncol(home_numerical)] # we will perform the PCA analysis on this dataset. 
```

```{r, echo=TRUE}
pca.train = home_numerical_features
prin_comp = prcomp(pca.train, center = TRUE, scale. = TRUE)

# Compute standard deviation of each principal component
std_dev = prin_comp$sdev

# Compute variance
pr_var = std_dev^2
prop_varex = pr_var/sum(pr_var)

# Cumulative scree plot
plot(cumsum(prop_varex), 
     xlab = "Principal Component", 
     ylab = "Cumulative Proportion of Variance Explained",
     type = "b")

abline(h=1, col='red')
abline(v=23, col='black', lty = 3)
```
23 of our 28 numerical features account for 99% of the variance.  
For 20 features, we still get an acceptable explained variance of ~96% > 95%.
  
  
3. Factor analysis  
For our first contact with the factors, let us get a closer look at level repartition to get an idea of fragmentation within the factors.  
```{r, echo=TRUE}
par(mfrow=c(1,3))
# Getting our factors
factors = unlist(lapply(home, is.factor))
home_factors = home[,factors]

# Street 
barplot(table(home_factors$Street)[order(table(home_factors$Street))], main = "Street")
# Neighbourhood
barplot(table(home_factors$Neighborhood)[order(table(home_factors$Neighborhood))], main = "Neighbourhood")
# Garage Finish
barplot(table(home_factors$GarageFinish)[order(table(home_factors$GarageFinish))], main = "GarageFinish")
```
Observing the factor columns, we see three types of repartitions.  
i) Clear surrepresentation of one level versus the others. 
In Street and Utilities, which are binary, this is conspicuous. From this we infer that these factors won't be very useful in predicting house price in general: nearly all houses will be in the same category along these factors (and for those who aren't, data is too sparse to generalise well).  
Note that this is also the case for RoofMatl, Heating, BsmtFinType2, Electrical, GarageCond, GarageQual.  
  
ii) High cardinality in the number of levels: this is especially the case for neighbourhood, which has 25 levels. We will regroup some of these levels together to improve our predictions.
Note that this is also the case for Exterior1st, Exterior2nd, BsmtExposure. 
  
iii) Classic factor repartition with reasonable representation of each modality, as is the case for Housestyle for instance. 
Note that this is also the case for HeatingQC, GarageFinish, BsmtFinType1.  

Intuitively, we said that both location and overall quality will impact SalePrice significantly. Let us check this with anova and ancova tests. 
```{r}
# Area
par(mfrow=c(1,2))
library(ggplot2)
ggplot(home, 
       aes(x=MSZoning, 
           y=SalePrice, 
           colour=MSZoning, 
           group = MSZoning, 
           fill = MSZoning)) + geom_boxplot(alpha=0.5, outlier.alpha=0) +geom_jitter(width=0.1)

# Overall quality 
ggplot(home, 
       aes(x=OverallQual, 
           y=SalePrice, 
           colour= OverallQual, 
           group = OverallQual, 
           fill = OverallQual)) + geom_boxplot(alpha=0.5, outlier.alpha=0) +geom_jitter(width=0.1)

# Year sold
boxplot(SalePrice~YrSold, data = home, xlab = "Year Sold", ylab = "SalePrice")
```
It appears that houses from the RM and RH areas are less expensive than the ones from FV and RL areas.
```{r}
# Anova on area
mod1<-lm(SalePrice ~ MSZoning-1 , data=home)
anova(mod1)

# Anova on quality
mod2<-lm(SalePrice ~ OverallQual-1 , data=home)
anova(mod2)
```
Anova tests seem to show that both area and overall quality have a strong effect on SalePrice. However, here we have not accounted for the interaction between MSzoning and overall quality: to validate our conclusions, we must show that it is not significant with an ANCOVA test. 
```{r}
# Ancova on quality and area
mod3 = lm(SalePrice~1 + MSZoning + OverallQual + OverallQual:MSZoning, data = home)
anova(mod3)
```
Since the last p-value is big, we will consider that SalePrice will depend in a similar manner on OverallQual and MSZoning. 

```{r}
home_factors
```

```{r}
colnames(home_factors)
```

```{r}
# Ancova between HeatingQC and Heating
mod3 = lm(SalePrice~1 + HeatingQC + Heating + HeatingQC:Heating, data = home)
anova(mod3)
```

```{r}
# Ancova between CentralAir and Heating
mod3 = lm(SalePrice~1 + CentralAir + Heating + CentralAir:Heating, data = home)
anova(mod3)
```
```{r}
# Ancova between CentralAir and Electrical
mod3 = lm(SalePrice~1 + HouseStyle + Electrical + HouseStyle:Electrical, data = home)
anova(mod3)
```






\textbf{III. Modeling and diagnostics}  
```{r}
# Making sure our data exploration didn't modify the dataset
home <- read.csv("train_preprocessed.csv", row.names=1)
```

Given the volatility of Saleprice, we will work on its logarithm to smooth its distribution and improve estimated residuals homoscedasticity. 
After manually removing poorly informative factors and other highly correlated numerical features, we start by fitting a full linear model to predict log(SalePrice) and check whether the postulates are validated. 

XXX and then add a Lasso penalization as we have a lot of regressors. Using those two models,the variable selection methods will enable us to select the appropriate regressors.  
Then, we will compare our models using criteria seen in class. We will also test the gaussian assumption of our model.  

Looking at our dataframe, we realised that some factor columns had a lot of differents modalities, with some that appeared very few times. Thus, for more efficiency of our algorithm, we decided to group them. We performed this for Neighbourhoood, RoofStyle, Condition1 and OverallQual.
```{r}
# Some elementary pruning and regrouping of categories
# Regrouping neighbourhoods of less than 20 and 50 sales together 
table_neighborhood= table(home$Neighborhood)
table_neighborhood[order(table_neighborhood)]

levels(home$Neighborhood) <- c(levels(home$Neighborhood), "N_Under20Sales","N_Under50Sales") 
home$Neighborhood[home$Neighborhood %in% c("Blueste","NPkVill","Veenker","BrDale","Blmngtn","MeadowV")] = "N_Under20Sales"
home$Neighborhood[home$Neighborhood %in% c("StoneBr","SWISU","ClearCr","IDOTRR","Timber","NoRidge","Mitchel")] = "N_Under50Sales"
```
```{r}
# Roofstyle
table(home$RoofStyle)[order(table(home$RoofStyle))]
levels(home$RoofStyle) <- c(levels(home$RoofStyle), "RS_Other") 
home$RoofStyle[home$RoofStyle %in% c("Shed","Mansard","Gambrel","Flat")] = "RS_Other"

# Condition 1
table(home$Condition1)[order(table(home$Condition1))]
levels(home$Condition1) <- c(levels(home$Condition1), "C_Other") 
home$Condition1[home$Condition1 %in% c("RRNe","RRNn","PosA","RRAe","PosN","RRAn","Artery")] = "C_Other"

# ExterCond
table(home$ExterCond)[order(table(home$ExterCond))]
levels(home$ExterCond) <- c(levels(home$ExterCond), "EC_Other") 
home$ExterCond[home$ExterCond %in% c("Po", "Ex", "Fa", "Gd")] = "EC_Other"

# OverallQual
table(home$OverallQual)
levels(home$OverallQual) <- c(levels(home$OverallQual), "Very_Low") 
home$OverallQual[home$OverallQual %in% c("-4.27238846180047", "-3.28279659466729", "-2.38962301582022")] = "Very_Low"
```
  
Then, using previous analyses and deeper data exploration, we found at that some columns did not help to predict our model, as they could add redundancies or errors. Thus, we decided to remove them (e.g. Street, RoofMatl and HeatingQC). 
```{r, echo=TRUE}
col_to_remove = c("Street", "Utilities", "RoofMatl", "Condition2","Exterior1st", "Exterior2nd", "Electrical", "SaleCondition", "BsmtFinType2", "Heating","RoofStyle", "GarageCond", "ExterQual", "GarageFinish", "OpenPorchSF", "HeatingQC", "Functional")
for (name in col_to_remove){cat(which(colnames(home)==name)," ")}
```

```{r}
home_streamlined = home[-c(6 ,9 ,22, 14, 23, 24, 41, 67, 35, 38, 21, 60, 27, 56, 63, 39, 52)]
```

```{r, echo=TRUE}
# home_streamlined[c(186, 251, 667),1:ncol(home_streamlined)]
```
 
```{r, echo=TRUE}
full_model = lm(log(SalePrice)~., data = home_streamlined)
summary(full_model)
```
According to the T test above, the variables having the most impact to explain SalePrice are the ones describing: 
- the location of the home (e.g. MSZoning, Neighborhood)
- the area of the home (LotArea)
- overall quality and condition ( OverallQual, OverallCond)
- # Comment/reword the specificities of some aspects of the home such as the roof (e.g. RoofStyleShed, RoofMat) and security functions (e.g. Fireplaces).  
- the construction period (e.g. YearBuilt, YearRemodAdd)

Many variables could be removed from our model while marginally affecting its efficiency to explain SalePrice. 
```{r, echo=TRUE}
# Validating the postulates 
par(mfrow=c(3,3))
plot(full_model, which=1) # Errors are centered
plot(full_model, which=3) # Errors are homoscedastic
acf(residuals(full_model), main= "Auto-correlation plot") # Errors are non-correlated
plot(full_model, which=2) # Errors are gaussian
plot(full_model, which=5) # Cook's distance
```
  
Looking at the graph, we can validate our postules. However, to improve the efficiency of our model, we will perform a selection of variables based on forward, backward and both methods. 
  
```{r, echo=TRUE, message= FALSE, comment=FALSE}
# Reducing our number of features using the 3 selection methods seen in class (forward, backward, both)
library(MASS)
# Forward method :
linear_select_variables_forward = stepAIC(full_model, data=home_streamlined, trace=0, direction=c("forward"), verbose = FALSE)
#summary(linear_select_variables_forward)

# Backward method
linear_select_variables_backward = stepAIC(full_model,~., trace=0, direction=c("backward") )
#summary(linear_select_variables_backward)

# Both
linear_select_variables_both = stepAIC(full_model, trace=0, direction=c("both"))
#summary(linear_select_variables_both)
```

To choose among the three methods, we retrieve the AIC of each model and choose the one with the smallest AIC.  
```{r, echo=TRUE}
extractAIC(linear_select_variables_forward)
extractAIC(linear_select_variables_backward)
extractAIC(linear_select_variables_both)
```
  
According to these results, if we decide to run a linear regression, we would choose the model extracted by the backward or the both method as they have the same AIC, smaller than the one of the forward method. We choose arbitrarily the backward one. For this specific model, let's verify that the postulates are verified.   
```{r, echo=TRUE}
# Validate (or not) the postulates 
par(mfrow=c(3,3))
plot(linear_select_variables_backward, which=1) # P1
plot(linear_select_variables_backward, which=3) # P2
acf(residuals(linear_select_variables_backward), main= "Auto-correlation plot") # P3
plot(linear_select_variables_backward, which=2) # P4
plot(linear_select_variables_backward, which=5) # Cook
```
Looking at the graphs, the postulates are verified for this model.  
  
Now, we want to verify that we don't have outliers in our model. 
```{r, echo=TRUE}
library(car)
influenceIndexPlot(linear_select_variables_backward)
```
Cook's distance plot: according to the Cook's criteria, we don't observe any leverage point or regression outlier.  
Studentized residuals plot: all points are in-between -2 and 2, so according to this criteria, we don't have regression outliers.  
Bonferroni's plot : we notice that 13 points have a p-value inferior to 0.05, so they are outliers according to this criteria.  
Hat plot: two points (327 and 783) seem to be leverage points according to this criteria.  
  
We can also use an outlier test to detect outliers: 
```{r, echo=TRUE}
# Finding the outliers of our model
outlierTest(linear_select_variables_backward)
```
Using this test, 8 possible outliers are given. As they all have very small p-value, we can consider them as outliers and we could remove them.
  
```{r, echo=TRUE}
# Loaging the library
library(glmnet)

# Splitting the data into regressors and output 
x_vars <- model.matrix(log(SalePrice)~. , home_streamlined)[,-1]
y_var <- log(home_streamlined$SalePrice)
lambda_seq <- 10^seq(2, -2, by = -.1)

# Splitting the data into test and train
set.seed(86)
train = sample(1:nrow(x_vars), nrow(x_vars)/2)
x_test = (-train)
y_test = y_var[x_test]

cv_output <- cv.glmnet(x_vars[train,], y_var[train], 
            alpha = 1, lambda = lambda_seq)

# identifying best lamda
best_lam <- cv_output$lambda.min
```

```{r}
# Rebuilding the model with best lamda value identified
lasso_best <- glmnet(x_vars[train,], y_var[train], alpha = 1, lambda = best_lam)
pred <- predict(lasso_best, s = best_lam, newx = x_vars[x_test,])
```

```{r}
final <- cbind(y_var[x_test], pred)
# Checking the first six obs
# head(final)
```

```{r}
coef(lasso_best)
```

```{r}
library(glmnet)
set.seed(123)
cv.lasso <- cv.glmnet(x_vars, y_var, alpha = 1)
plot(cv.lasso)
```
```{r}
coef(cv.lasso, cv.lasso$lambda.1se)
```




\textbf{IV. Final models}  
  
Parameters of our model:  
Error:  
Confidence interval:  
p-value:  
Estimate of the generalization error of our final model:  
  
\textbf{V. Discussion}  
Final conclusions on our model:  
key learnings:  
Improvements for future projects:  