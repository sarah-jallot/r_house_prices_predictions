---
title: "Regression - Final project"
author: "Victoire de Termont and Sarah Jallot"
date: "12/3/2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

\textbf{I. Introduction} 
  
Our objective is to predict the prices of residential homes in Ames, Iowa accurately by exploiting an existing dataset to its best. Our key metric will be the RMSE on test data.     
The training data consists of a set of 66 categorical and quantitative variables extensively describing 1,460 residential homes in Ames, Iowa, and their corresponding price in dollars. 
We want to preprocess and explore our data smartly, select the best performing features, and fit the most adapted regression model after validating the key assumptions needed to implement it.      

We will first explore our data to get a first intuition of an efficient model. Preprocessing and understanding the data implies that we understand what we are given, check that there are no missing values, and than R correctly categorises each regressor.  
```{r, echo=TRUE}
# Loading the preprocessed dataset 
home <- read.csv("train_imputed.csv", row.names=1)
```

```{r, echo=TRUE}
head(home)
```
The data we are handling is heterogeneous, although it is presented in a structured manner. We are dealing both with categorical and quantitative variables, which we will have to preprocess separately.  
  
`````{r, echo=TRUE}
summary(home)
```
In our dataframe, there are 67 columns including the price, meaning a full model would include 66 features and an intercept, and 1,460 observations.  
The list of features is extensive: it is probable that not all regressors are useful to predict house price. 
  
The quantitative variables differ in scale and range: prices start from ~35,000 dollars, and can attain 755,000, whereas bedrooms above ground range from 1 to 8 only for instance. 
This means that scaling our data could optimise model performance.  
  
We have to treat missing values before doing analysis. 
```{r, echo=TRUE}
sum(is.na(home))
```
There are no missing values in our dataframe.  
  
```{r}
# Getting regressor types
#sapply(home, class)
str(home, give.attr = FALSE)
```
From this summary, R appears to classify some features as integers when they could be considered as factors: mainly features related to years or areas, like MSZoning ir YearBuilt. 
All quantitative features are integers and price, the output, is the only numeric.  
Factor variables are automatically numerically encoded by R, which assigns them to ascending levels using their alphabetical order. Note that this might bias the data by assigning a higher value to one level versus another without any ground to do so. 
```{r}
wrong_categories = c("YearBuilt", "YearRemodAdd", "GarageYrBlt", "MoSold", "YrSold", "MSZoning", "MSSubClass")
for (category in wrong_categories) {home[,category] = as.factor(home[,category])}
```
  
\textbf{II. Exploratory data analysis / initial modeling} 
  
To do correlation matrices, we need numerical or factor variables.  
  
Now that our dataset is correctly formated, we will extract trends and relationships among the regressors. Comparing them is also essential, for instance using pairplots.
As our intuition is that the location of the flat has a big impact, we will test this hypothesis.  
```{r}
# Doing a pairplot for 10 features 
pairs(home[1:10])
```
Getting a few insights from the data 
```{r}
plot(SalePrice ~ YrSold, data=home,  xlab="Year Sold", ylab="Sale Price")
```

# COMMENT
A. Numerical analysis
```{r}
# Plot histogram grid
nums = unlist(lapply(home, is.numeric))
home_numerical = (home[ , nums])
dim(home_numerical)
#home_numerical = as.data.frame(matrix(unlist(home_numerical), nrow=length(home_numerical), byrow=T))
```

```{r}
#hist(home_numerical)
```
Histograms are a good way to understand the behavior of each regressor. 
# COMMENT

Looking at inter-feature correlations first, we observe that OverallQual is correlated to may other features on the correlation matrix. This indicates that there is a link between it and other variables. This is encouraging as it means that we would be able to remove some columns without losing too much information.

The numerical features that are the most correlated with SalePrice are OverallQual, TotalBsmtSE, X1stElrSF, GrLivArea, GarageCars and GarageArea. On the contrary, MoSold, YrSold and BsmtHalfBath are poorly correlated to SalePrice. As the correlation matrix does not take into account interactions between our features in predicting sales price, we will not infer from this which variables we will keep in our final model.  

## PCA 
Given that we have 67 columns, of which 29 are quantitative, let's explore the possibilities we have to reduce our dataframe.  
Because PCA works best with (scaled) numerical data, we will perform it on our scaled numerical feature columns. 
```{r}
home_numerical_output = home_numerical$SalePrice
home_numerical_features = home_numerical[,2:ncol(home_numerical)] # we will perform the PCA analysis on this dataset. 
```


```{r}
pca.train = home_numerical_features
prin_comp = prcomp(pca.train, center = TRUE, scale. = TRUE)

# Compute standard deviation of each principal component
std_dev = prin_comp$sdev

# Compute variance
pr_var = std_dev^2
prop_varex = pr_var/sum(pr_var)

# Cumulative scree plot
plot(cumsum(prop_varex), 
     xlab = "Principal Component", 
     ylab = "Cumulative Proportion of Variance Explained",
     type = "b")

abline(h=1, col='red')
abline(v=23, col='black', lty = 3)
```
23 of our 28 numerical features account for 99% of the variance.  
For 20 features, we still get an acceptable explained variance of ~96% > 95%.
  
  
B. Factor analysis  
For our first contact with the factors, let us get a closer look at level repartition to get an idea of fragmentation within the factors.  
```{r}
par(mfrow=c(1,3))
# Plotting factor repartition 
factors = unlist(lapply(home, is.factor))
home_factors = home[,factors]
for (feature in 1:ncol(home_factors)){
barplot(table(home_factors[feature]), main = colnames(home_factors)[feature])}
```
Observing the factor columns, we see three types of repartitions.  
i) Clear surrepresentation of one level versus the others. 
In Street and Utilities, which are binary, this is conspicuous. From this we infer that these factors won't be very useful in predicting house price in general: nearly all houses will be in the same category along these factors (and for those who aren't, data is too sparse to generalise well).  
(this is also the case for RoofMatl, Heating, )
ii) High cardinality in the number of levels: this is especially the case for neighbourhood, which has 25 levels. We could regroup some of these levels together to improve our predictions.  
(this is also the case for Exterior1st, Exterior2nd, GarageCond)
  
iii) Classic factor repartition with


As we have many factor columns, we have to transform them into dummies in order to analyze the data. We will remove the ones that are too correlated.  
```{r}
plot(home_factors[1:10])
```

We implement dummy categorical encoding to get rid of the alphabetical levels automatically implemented by R.  
```{r}
library(dummies)
home_encoded <- dummy.data.frame(home)
head(home_encoded)
```
Let us perform PCA a second time, this time adding in our dummy-encoded factors. 
```{r}
home_encoded_features = home_encoded[,2:ncol(home_encoded)]
pca.train = home_encoded_features # excluding SalePrice from the PCA
prin_comp = prcomp(pca.train, center = TRUE, scale. = TRUE)

# Compute standard deviation of each principal component
std_dev = prin_comp$sdev

# Compute variance
pr_var = std_dev^2
prop_varex = pr_var/sum(pr_var)

# Cumulative scree plot
plot(cumsum(prop_varex), 
     xlab = "Principal Component", 
     ylab = "Cumulative Proportion of Variance Explained",
     type = "b")

abline(h=1, col='red')
abline(v=200, col='black', lty = 3)
```
Keeping 200 of the 262 features, we explain 99% of the variance. Keeping 170 we explain ~96% > 95% of the variance. A priori, we can perform dimensionality reduction without loosing substantial information. 

```{r}
# Example of very correlated feature : location of the home -> do a boxplot per location MSZoning
library(ggplot2)
ggplot(home) + aes(x=MSZoning, y=SalePrice) + geom_boxplot()
```

```{r}
# Example of not correlated feature : condition of the garage 
ggplot(home) + aes(x=GarageCond, y=SalePrice) + geom_boxplot()
```


```{r}
# Try to find the law of the most correlated variables
par(mfrow=c(1,2))

hist(home$OverallQual)
d = density(home$OverallQual)
lines(d, col='red')

uniform = runif(n=1640,min = 1, max=10)
qqplot(home$OverallQual, uniform) # does not follow a uniform law but not a normal law either
```


```{r}
# pairplot split by location MSZoning ? 
library(GGally)
ggpairs(home, aes(colour = MSZoning, alpha = 0.8), title="Pairs plot for home dataset") + theme_grey(base_size = 8)
```

\textbf{III. Modeling and diagnostics}  
  
We will start by a linear model, and then add a Lasso penalization as we have a lot of regressors. Using those two models, Anova methods will enable us to select the appropriate regressors.  
Then, we will compare our models using criteria seen in class. We will also test the gaussian assumption of our model.  

```{r}
# Define a linear model using all features 
reg=lm(SalePrice~., data=home)
summary(reg)
```


```{r}
# 3 anova tests with a classical linear model


```

```{r}
# Validate (or not) the postulates 
par(mfrow=c(2,2))
plot(reg, which=1) # P1
plot(reg, which=3) # P2
acf(residuals(reg), main= "Auto-correlation plot") # P3
plot(reg, which=2) # P4
```

```{r}
# tests to detect outliers
```
  
```{r}
# Compare the models with and without outliers (AIC...)
```
  
\textbf{IV. Final models}  
  
Parameters of our model:  
Error:  
Confidence interval:  
p-value:  
Estimate of the generalization error of our final model:  
  
\textbf{V. Discussion}  
Final conclusions on our model:  
key learnings:  
Improvements for future projects:  