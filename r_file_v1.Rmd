---
title: "Regression - Final project"
author: "Victoire de Termont and Sarah Jallot"
date: "12/3/2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

\textbf{I. Introduction} 
  
Our objective is to predict the prices of residential homes in Ames, Iowa accurately by exploiting an existing dataset to its best. Our key metric will be the RMSE on test data.     
The training data consists of a set of 66 categorical and quantitative variables extensively describing 1460 residential homes in Ames, Iowa, and their corresponding price in dollars. 
We want to preprocess and explore our data smartly, select the best performing features, and fit the most adapted regression model after validating the key assumptions needed to implement it.      

We will first explore our data to get a first intuition of an efficient model. Preprocessing and understanding the data implies that we understand what we are given, check that there are no missing values, and than R correctly categorises each regressor.  
```{r, echo=TRUE}
# Loading the data set 
home <- read.csv("train_imputed.csv", row.names=1)
```

```{r, echo=TRUE}
# Looking at the dataset 
head(home)
```
The data we are presented with is heterogeneous. We are dealing both with categorical and quantitative variables, which we will have to preprocess separately.  
`````{r, echo=TRUE}
summary(home)
```
In our dataframen there are 67 columns including the price, meaning a full model would include 66 features and an intercept, and 1,460 observations.  
The list of features is extensive: we can infer that not all regressors are useful to predict house price. 
  
The quantitative variables differ in scale and range: prices start from ~35,000 dollars, and can attain 755,000, whereas bedrooms above ground range from 1 to 8 only for instance. 
This means that scaling our data could be an option to optimise model performance. Note that all quantitative features seem to be integers.  

We have to treat missing values before doing analysis. 
```{r, echo=TRUE}
sum(is.na(home))
```
There are no missing values in our dataframe.
To do correlation matrices, we need numerical or factor variables. 
Looking at the type of each regressor, our dataframe seems to classify them correctly. 
```{r}
# Getting the type of our regressors
#sapply(home, class)
# Other method 
str(home, give.attr = FALSE)
```

As we have many factor columns, we have to transform them into dummies in order to analyze the data. Also, as we have a lot of columns, we will remove the ones that are too correlated among them.


\textbf{II. Exploratory data analysis / initial modeling}  
```{r}
#Given that our features are very highly correlated, when fitting our model, we can assume leaving some out will marginally affect model performance.  
#Performing PCA on our dataset, then looking at the proportion of variance explained, allows us to confirm our assumption and quantify how many features are useful to use. 
# Getting numerical features from our dataset
nums = unlist(lapply(home, is.numeric)) 
home_numerical = home[ , nums]
print(home_numerical)
pca.train = home[2:29,]
# 
prin_comp = prcomp(pca.train, scale. = T)

# Compute standard deviation of each principal component
std_dev = prin_comp$sdev

# Compute variance
pr_var = std_dev^2
prop_varex = pr_var/sum(pr_var)

# Cumulative scree plot
plot(cumsum(prop_varex), xlab = "Principal Component",
              ylab = "Cumulative Proportion of Variance Explained",
              type = "b")
```
  
Now that our dataset is correctly formated, we can try to extract trends and relationships among the regressors. Indeed, histograms are a good way to understand the behavior of each regressor. Comparing them is also essential, for instance using pairplots. As the goal is to have an efficient model, we don't want to keep variables that are not correlated with the output, or that are very correlated among them as it leads to redundance. Thus, we could get a first intuition of variables to keep using a correlarion matrix. We will have to deal string regressors before doing so.  
As our intuition is that the location of the flat has a big impact, we will try to mathematically test this hypothesis.  

```{r}
# Doing a pairplot for 10 features 
pairs(home[1:10])
```

```{r}
# Doing a correlation matrix using only our numerical features 
nums = unlist(lapply(home, is.numeric)) 
home_numerical = home[ , nums]
library(corrplot) 
R = round(cor(home_numerical),2)
corrplot(R, method="ellipse")
```
Using a correlation matrix, the numerical features that are the most correlated with SalePrice are OverallQual, TotalBsmtSE, X1stElrSF, GrLivArea, GarageCars and GarageArea.
On the contrary, MoSold, YrSold and BsmtHalfBath seem very poorly correlated with SalePrice. 
These correlations allow us to get a first intuition on the variables that we will keep to build our regression model. Indeed, we want our features to explain the output, and thus we will remove the column that don't serve that purpose. 

We should also look at correlation among our features. Indeed, OverallQual is correlated to may other features on the correlation matrix. This indicates that there is a link between it and other variables. This is encouraging as it means that we would be able to remove some columns without losing too much information.


```{r}
# getting only numerical and factor variables to be able to use our full model 
library(dummies)
home_dummy <- dummy.data.frame(home)
# removing the first dummy columnn for each category : CA NE MARCHE PAS 

head(home_dummy)
```



```{r}
# Example of very correlated feature : location of the home -> do a boxplot per location MSZoning
library(ggplot2)
ggplot(home) + aes(x=MSZoning, y=SalePrice) + geom_boxplot()
```

```{r}
# Example of not correlated feature : condition of the garage 
ggplot(home) + aes(x=GarageCond, y=SalePrice) + geom_boxplot()
```


```{r}
# Try to find the law of the most correlated variables
par(mfrow=c(1,2))

hist(home$OverallQual)
d = density(home$OverallQual)
lines(d, col='red')

uniform = runif(n=1640,min = 1, max=10)
qqplot(home$OverallQual, uniform) # does not follow a uniform law but not a normal law either
```


```{r}
# pairplot split by location MSZoning ? 
library(GGally)
ggpairs(home, aes(colour = MSZoning, alpha = 0.8), title="Pairs plot for home dataset") + theme_grey(base_size = 8)
```

\textbf{III. Modeling and diagnostics}  
  
We will start by a linear model, and then add a Lasso penalization as we have a lot of regressors. Using those two models, Anova methods will enable us to select the appropriate regressors.  
Then, we will compare our models using criteria seen in class. We will also test the gaussian assumption of our model.  

```{r}
# Define a linear model using all features 
reg=lm(SalePrice~., data=home)
summary(reg)
```


```{r}
# 3 anova tests with a classical linear model


```

```{r}
# Validate (or not) the postulates 
par(mfrow=c(2,2))
plot(reg, which=1) # P1
plot(reg, which=3) # P2
acf(residuals(reg), main= "Auto-correlation plot") # P3
plot(reg, which=2) # P4
```

```{r}
# tests to detect outliers
```
  
```{r}
# Compare the models with and without outliers (AIC...)
```
  
\textbf{IV. Final models}  
  
Parameters of our model:  
Error:  
Confidence interval:  
p-value:  
Estimate of the generalization error of our final model:  
  
\textbf{V. Discussion}  
Final conclusions on our model:  
key learnings:  
Improvements for future projects:  