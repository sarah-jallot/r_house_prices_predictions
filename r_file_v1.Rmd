---
title: "Regression - Final project"
author: "Victoire de Termont and Sarah Jallot"
date: "12/3/2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

\textbf{I. Introduction}  
  
We are given a dataset, containing variables that describle almost comprehensively every aspect of residential homes in Ames, Iowa. For a certain number of homes (the number of lines of our dataframe), the associated price is given. The goal of this analysis is to select the good criteria and the most adapted regression model in order to be able to dertermine the price of other homes in Ames, based on this information.  
Firt, we have to look at the data, and get a first intuition of an efficient model. Indeed, before starting our full analysis, we have to understand what we are given, check that there are no missing values, and than R classifies correctly each regressor.  

```{r, echo=TRUE}
# Loading the data set 
home <- read.csv("train_imputed.csv", row.names=1)
```

```{r, echo=TRUE}
# Looking at the dataset 
head(home)
```
```{r, echo=TRUE}
summary(home)
```

Looking at our dataframe, we have 67 columns, thus 66 regressors (and then we have to include an intercept) and 1460 observations. Our dataframe seems to contain almost each aspect of a home, but we can imagine that they are not all useful to predict the price. 
We have to treat missing values before doing analysis. 
```{r, echo=TRUE}
sum(is.na(home))
```
There is no missing value in our dataframe.
To do correlation matrices, we need numerical or factor variables. 

```{r}
# Getting the type of our regressors
sapply(home, class)
```
```{r}
# Other method 
str(home, give.attr = FALSE)
```

Looking at the type of each regressor, our dataframe seems to classify them correctly. As we have many factor columns, we have to transform them into dummies in order to analyze the data. Also, as we have a lot of columns, we will remove the ones that are too correlated among them.


\textbf{II. Exploratory data analysis / initial modeling}  
  
Now that our dataset is correctly formated, we can try to extract trends and relationships among the regressors. Indeed, histograms are a good way to understand the behavior of each regressor. Comparing them is also essential, for instance using pairplots. As the goal is to have an efficient model, we don't want to keep variables that are not correlated with the output, or that are very correlated among them as it leads to redundance. Thus, we could get a first intuition of variables to keep using a correlarion matrix. We will have to deal string regressors before doing so.  
As our intuition is that the location of the flat has a big impact, we could try to mathematically test this hypothesis.  

```{r}
# Doing a pairplot for 10 features 
pairs(home[1:10])
```

```{r}
# Doing a correlation matrix using only our numerical features 
nums = unlist(lapply(home, is.numeric)) 
home_numerical = home[ , nums]
library(corrplot) 
R = round(cor(home_numerical),2)
corrplot(R, method="ellipse")
```
Using a correlation matrix, the numerical features that are the most correlated with SalePrice are OverallQual, TotalBsmtSE, X1stElrSF, GrLivArea, GarageCars and GarageArea.
On the contrary, MoSold, YrSold and BsmtHalfBath seem very poorly correlated with SalePrice. 
These correlations allow us to get a first intuition on the variables that we will keep to build our regression model. Indeed, we want our features to explain the output, and thus we will remove the column that don't serve that purpose. 

We should also look at correlation among our features. Indeed, OverallQual is correlated to may other features on the correlation matrix. This indicates that there is a link between it and other variables. This is encouraging as it means that we would be able to remove some columns without losing too much information.


```{r}
# getting only numerical and factor variables to be able to use our full model 
library(dummies)
home_dummy <- dummy.data.frame(home)
# removing the first dummy columnn for each category : CA NE MARCHE PAS 

head(home_dummy)
```



```{r}
# Example of very correlated feature : location of the home -> do a boxplot per location MSZoning
library(ggplot2)
ggplot(home) + aes(x=MSZoning, y=SalePrice) + geom_boxplot()
```

```{r}
# Example of not correlated feature : condition of the garage 
ggplot(home) + aes(x=GarageCond, y=SalePrice) + geom_boxplot()
```


```{r}
# Try to find the law of the most correlated variables
par(mfrow=c(1,2))

hist(home$OverallQual)
d = density(home$OverallQual)
lines(d, col='red')

uniform = runif(n=1640,min = 1, max=10)
qqplot(home$OverallQual, uniform) # does not follow a uniform law but not a normal law either
```


```{r}
# pairplot split by location MSZoning ? 
library(GGally)
ggpairs(home, aes(colour = MSZoning, alpha = 0.8), title="Pairs plot for home dataset") + theme_grey(base_size = 8)
```

\textbf{III. Modeling and diagnostics}  
  
We will start by a linear model, and then add a Lasso penalization as we have a lot of regressors. Using those two models, Anova methods will enable us to select the appropriate regressors.  
Then, we will compare our models using criteria seen in class. We will also test the gaussian assumption of our model.  

```{r}
# Define a linear model using all features 
reg=lm(SalePrice~., data=home)
summary(reg)
```


```{r}
# 3 anova tests with a classical linear model


```

```{r}
# Validate (or not) the postulates 
par(mfrow=c(2,2))
plot(reg, which=1) # P1
plot(reg, which=3) # P2
acf(residuals(reg), main= "Auto-correlation plot") # P3
plot(reg, which=2) # P4
```

```{r}
# tests to detect outliers
```
  
```{r}
# Compare the models with and without outliers (AIC...)
```
  
\textbf{IV. Final models}  
  
Parameters of our model:  
Error:  
Confidence interval:  
p-value:  
Estimate of the generalization error of our final model:  
  
\textbf{V. Discussion}  
Final conclusions on our model:  
key learnings:  
Improvements for future projects:  