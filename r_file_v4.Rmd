---
title: "Regression - Final project"
author: "Victoire de Termont and Sarah Jallot"
date: "12/3/2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

\textbf{I. Introduction} 
```{r}
load("DataProject.RData")
```
Our objective is to predict the prices of residential homes in Ames, Iowa accurately by exploiting an existing dataset to its best. Our key metric will be the RMSE on test data.     
The training data consists of a set of 66 categorical and quantitative variables extensively describing 1,095 residential homes in Ames, Iowa, and their corresponding price in dollars. 
We want to preprocess and explore our data smartly, select the best performing features, and fit the most adapted regression model after validating the key assumptions needed to implement it.     
  
We will first explore our data to get intuition of an efficient model. Understanding and preprocessing the data implies that we understand what we are given, check that there are no missing values, and that R correctly categorises each regressor.  
  
```{r, include = FALSE}
# Loading the preprocessed dataset 
home = train[,2:ncol(train)]
#head(home)
```
The data we are handling is heterogeneous, although it is presented in a structured manner. We are dealing both with categorical and quantitative variables, which we will have to preprocess separately.  
  
`````{r, include = FALSE } 
summary(home)
```
  
Displaying the summary of our dataframe, there are 67 columns including the price, meaning a full model would include 66 features and an intercept, and 1,095 observations.  
The list of features is extensive: it is probable that not all regressors are useful to predict house price.  
  
The quantitative variables differ in scale and range: prices start from ~35,000 dollars, and can attain 755,000, whereas bedrooms above ground range from 1 to 8 only for instance.  
This means that scaling our data could optimise model performance.  
  
We first treat missing values before launching into the analysis.  
  
```{r}
cat("There are", sum(is.na(home)), "missing values in our dataframe.")
```

```{r}
# Getting regressor types
#str(home, give.attr = FALSE)
```
First intuition: looking at this dataset, we can see that we have a lot of regressors and that some might be more relevant than others to explain the SalePrice. For instance, the variable MSZoning is expected to have much more impact on the price that the variable Heating, as the heating system is something that can be changed, whereas the location is permanent.  
Thus, we will perform different types of analysis to keep only the most relevant variables in our model. 

Additionally, R appears to classify some features as integers when they could be considered as factors: mainly factors related to areas and quality ratings. After debating whether we would consider Years as categories, we decided against given that we might come across a previously unencountered year in the test data which would nullify the interest of using year as a factor.
  
All quantitative features are integers and price, the output, is the only numeric.  
Factor variables are automatically numerically encoded by R, which assigns them to ascending levels using their alphabetical order. Note that this might bias the data by assigning a higher value to one level versus another without any ground to do so.  
```{r}
home$OverallQual = as.factor(home$OverallQual)
home$OverallCond = as.factor(home$OverallCond)
home$MoSold = as.factor(home$MoSold)
home$MSSubClass = as.factor(home$MSSubClass)
home$Fireplaces = as.factor(home$Fireplaces)
# head(home)
```

```{r}
# str(home)
```

\textbf{II. Exploratory data analysis / initial modeling} 

1. Numerical analysis  
We will extract trends and relationships among the regressors. Comparing them is also essential, for instance using pairplots.
```{r}
# Extracting all numerical variables
nums = unlist(lapply(home, is.numeric))
home_numerical = (home[ , nums])
```

We first set out to observe the output, SalePrice. 
```{r}
par(mfrow=c(1,2))
# A few observations on SalePrice. 
hist(home_numerical$SalePrice, main = "SalePrice")
boxplot(home_numerical$SalePrice, main = "SalePrice")
```

```{r}
cat("The mean of Saleprice is", mean(home_numerical$SalePrice), "whereas the median is higher at", median(home_numerical$SalePrice), "indicating that extreme values skew our data to the right.")
```
SalePrice is volatile with many houses to the left hand side, but with an important number of outliers to the right with extreme values which skew the data. To smoothen the output and approach a normal distribution, we will consider the log when fitting our model. 
```{r}
par(mfrow=c(1,3))
hist(log(home_numerical[,"SalePrice"]))
boxplot(log(home_numerical[,"SalePrice"]))
qqnorm(log(home_numerical$SalePrice))
```
log(SalePrice) is pretty close to a normal distribution, except for extreme values. 

```{r}
# Correlation matrix for numerical features
library(corrplot)
R = round(cor(home_numerical),2)
corrplot(R, method='ellipse')
```
Our intuition is that three feature types mainly drive SalePrices: location, area/surface and quality. Let's start by analysing the numerical variable LotArea.  
```{r}
par(mfrow=c(1,2))
# Area
hist(home_numerical$LotArea, main = "LotArea")
boxplot(home_numerical$LotArea, main = "LotArea")
```
We believe that areas, particularly LotArea, will be important when predicting SalePrice. 
Most areas are located within [-2,2] in our pre-processed dataset, with many outliers towards the left- or the right-hand-side. 

Looking at inter-feature correlations first, we observe that OverallQual is correlated to may other features on the correlation matrix. This indicates that there is a link between it and other variables. This is encouraging as it means that we would be able to remove some columns without losing too much information.

The numerical features that are the most correlated with SalePrice are OverallQual, TotalBsmtSE, X1stElrSF, GrLivArea, GarageCars and GarageArea. On the contrary, MoSold, YrSold and BsmtHalfBath are poorly correlated to SalePrice. As the correlation matrix does not take into account interactions between our features in predicting sales price, we will not infer from this which variables we will keep in our final model.  

## PCA 
Given that we have 67 columns, of which 29 are quantitative, let's explore the possibilities we have to reduce our dataframe.  
Because PCA works best with (scaled) numerical data, we will perform it on our scaled numerical feature columns. 
```{r}
home_numerical_output = home_numerical$SalePrice
home_numerical_features = home_numerical[,2:ncol(home_numerical)] # we will perform the PCA analysis on this dataset. 
```

```{r}
pca.train = home_numerical_features
prin_comp = prcomp(pca.train, center = TRUE, scale. = TRUE)

# Compute standard deviation of each principal component
std_dev = prin_comp$sdev

# Compute variance
pr_var = std_dev^2
prop_varex = pr_var/sum(pr_var)

# Cumulative scree plot
plot(cumsum(prop_varex), 
     xlab = "Principal Component", 
     ylab = "Cumulative Proportion of Variance Explained",
     type = "b")

abline(h=1, col='red')
abline(v=23, col='black', lty = 3)
```
23 of our 28 numerical features account for 99% of the variance.  
For 20 features, we still get an acceptable explained variance of ~96% > 95%.
  
  
3. Factor analysis  
For our first contact with the factors, let us get a closer look at level repartition to get an idea of fragmentation within the factors.  
```{r}
par(mfrow=c(1,3))
# Getting our factors
factors = unlist(lapply(home, is.factor))
home_factors = home[,factors]

# Street 
barplot(table(home_factors$Street)[order(table(home_factors$Street))], main = "Street")
# Neighbourhood
barplot(table(home_factors$Neighborhood)[order(table(home_factors$Neighborhood))], main = "Neighbourhood")
# Garage Finish
barplot(table(home_factors$GarageFinish)[order(table(home_factors$GarageFinish))], main = "GarageFinish")
```
Observing the factor columns, we see three types of repartitions.  
i) Clear surrepresentation of one level versus the others. 
In Street and Utilities, which are binary, this is conspicuous. From this we infer that these factors won't be very useful in predicting house price in general: nearly all houses will be in the same category along these factors (and for those who aren't, data is too sparse to generalise well).  
Note that this is also the case for RoofMatl, Heating, BsmtFinType2, Electrical, GarageCond, GarageQual.  
  
ii) High cardinality in the number of levels: this is especially the case for neighbourhood, which has 25 levels. We will regroup some of these levels together to improve our predictions.
Note that this is also the case for Exterior1st, Exterior2nd, BsmtExposure. 
  
iii) Classic factor repartition with reasonable representation of each modality, as is the case for Housestyle for instance. 
Note that this is also the case for HeatingQC, GarageFinish, BsmtFinType1.  

Intuitively, we said that both location and overall quality will impact SalePrice significantly. Let us check this with anova and ancova tests. 
```{r}
# Area
par(mfrow=c(1,2))
library(ggplot2)
ggplot(home, 
       aes(x=MSZoning, 
           y=SalePrice, 
           colour=MSZoning, 
           group = MSZoning, 
           fill = MSZoning)) + geom_boxplot(alpha=0.5, outlier.alpha=0) +geom_jitter(width=0.1)

# Overall quality 
ggplot(home, 
       aes(x=OverallQual, 
           y=SalePrice, 
           colour= OverallQual, 
           group = OverallQual, 
           fill = OverallQual)) + geom_boxplot(alpha=0.5, outlier.alpha=0) +geom_jitter(width=0.1)

# Year sold
boxplot(SalePrice~YrSold, data = home, xlab = "Year Sold", ylab = "SalePrice")
```

It appears that houses from the RM and RH areas are less expensive than the ones from FV and RL areas.
```{r}
# Anova on area
mod1<-lm(SalePrice ~ MSZoning-1 , data=home)
anova(mod1)

# Anova on quality
mod2<-lm(SalePrice ~ OverallQual-1 , data=home)
anova(mod2)
```
Anova tests seem to show that both area and overall quality have a strong effect on SalePrice. However, here we have not accounted for the interaction between MSzoning and overall quality: to validate our conclusions, we must show that it is not significant with an ANCOVA test. 
```{r}
# Ancova on quality and area
mod3 = lm(SalePrice~1 + MSZoning + OverallQual + OverallQual:MSZoning, data = home)
anova(mod3)
```
Since the last p-value is big, we will consider that SalePrice will depend in a similar manner on OverallQual and MSZoning. 


```{r}
# Ancova between HeatingQC and Heating
mod3 = lm(SalePrice~1 + HeatingQC + Heating + HeatingQC:Heating, data = home)
anova(mod3)
```

```{r}
# Ancova between CentralAir and Heating
mod3 = lm(SalePrice~1 + CentralAir + HeatingQC + CentralAir:HeatingQC, data = home)
anova(mod3)
```

```{r}
# Ancova between CentralAir and Heating
mod3 = lm(SalePrice~1 + CentralAir + Heating + HeatingQC + CentralAir:Heating + CentralAir:HeatingQC + Heating:HeatingQC + CentralAir:Heating:HeatingQC , data = home)
anova(mod3)
```

```{r}
# Ancova between CentralAir and Electrical
mod3 = lm(SalePrice~1 + HouseStyle + Electrical + HouseStyle:Electrical, data = home)
anova(mod3)
```



\textbf{III. Modeling and diagnostics}  

Given the volatility of Saleprice, we will work on its logarithm to smooth its distribution and improve estimated residuals homoscedasticity. 
After manually removing poorly informative factors and other highly correlated numerical features, we start by fitting a full linear model to predict log(SalePrice) and check whether the postulates are validated. 

XXX and then add a Lasso penalization as we have a lot of regressors. Using those two models,the variable selection methods will enable us to select the appropriate regressors.  
Then, we will compare our models using criteria seen in class. We will also test the gaussian assumption of our model.  

Looking at our dataframe, we realised that some factor columns had a lot of differents modalities, with some that appeared very few times. Thus, for more efficiency of our algorithm, we decided to group them. We performed this for Neighbourhoood, RoofStyle, Condition1 and OverallQual.
```{r, include=FALSE}
# Some elementary pruning and regrouping of categories
# Regrouping neighbourhoods of less than 20 and 50 sales together 
table_neighborhood= table(home$Neighborhood)
table_neighborhood[order(table_neighborhood)]

levels(home$Neighborhood) <- c(levels(home$Neighborhood), "N_Under20Sales","N_Under50Sales") 
home$Neighborhood[home$Neighborhood %in% c("Blueste","NPkVill","Veenker","BrDale","Blmngtn","MeadowV")] = "N_Under20Sales"
home$Neighborhood[home$Neighborhood %in% c("StoneBr","SWISU","ClearCr","IDOTRR","Timber","NoRidge","Mitchel")] = "N_Under50Sales"
```

```{r, include=FALSE}
# Roofstyle
table(home$RoofStyle)[order(table(home$RoofStyle))]
levels(home$RoofStyle) <- c(levels(home$RoofStyle), "RS_Other") 
home$RoofStyle[home$RoofStyle %in% c("Shed","Mansard","Gambrel","Flat")] = "RS_Other"

# Condition 1
table(home$Condition1)[order(table(home$Condition1))]
levels(home$Condition1) <- c(levels(home$Condition1), "C_Other") 
home$Condition1[home$Condition1 %in% c("RRNe","RRNn","PosA","RRAe","PosN","RRAn","Artery")] = "C_Other"

# ExterCond
table(home$ExterCond)[order(table(home$ExterCond))]
levels(home$ExterCond) <- c(levels(home$ExterCond), "EC_Other") 
home$ExterCond[home$ExterCond %in% c("Po", "Ex", "Fa", "Gd")] = "EC_Other"

# OverallQual
table(home$OverallQual)
levels(home$OverallQual) <- c(levels(home$OverallQual), "Very_Low") 
home$OverallQual[home$OverallQual %in% c("-4.27238846180047", "-3.28279659466729", "-2.38962301582022")] = "Very_Low"
```

```{r}
# Ancova on GarageCond and GarageFinish
mod3 = lm(SalePrice~1 + GarageCond + GarageFinish + GarageCond:GarageFinish, data = home)
anova(mod3)
```

```{r}
# Ancova on GarageCond and GarageQual
mod3 = lm(SalePrice~1 + GarageCond + GarageQual + GarageCond:GarageQual, data = home)
anova(mod3)
```

```{r}
# Ancova on GarageCond and OverallCond
mod3 = lm(SalePrice~1 + GarageCond + OverallCond + GarageCond:OverallCond, data = home)
anova(mod3)
```

```{r}
# Ancova on ExterQual and OverallCond
mod3 = lm(SalePrice~1 + ExterQual + OverallCond + ExterQual:OverallCond, data = home)
anova(mod3)
```  

```{r}
# Ancova on OverallQual and HeatingQC
mod3 = lm(SalePrice~1 + OverallQual + HeatingQC + OverallQual:HeatingQC, data = home)
anova(mod3)
```
```{r}
# Ancova on BldgType and Exterior1st
mod3 = lm(SalePrice~1 + BldgType + Exterior1st + BldgType:Exterior1st, data = home)
anova(mod3)
```

Then, using previous analyses and deeper data exploration, we found at that some columns did not help to predict our model, as they could add redundancies or errors. Thus, we decided to remove them:   
- We removed some features that took almost a single value for all oberservations: Street, Utilities, RoofMatl, Condition2, Heating, Electrical, Functional, GarageCond.  
- For some features, we were not sure wether or not they had an influence, so we tested the model with and without them and removed them if they were not improving our score : HeatingQC, SaleCondition, BsmtFinType2, RoofStyle, ExterQual.  
- We also removed some features based on Anova tests: Exterior1st, Exterior2nd, GarageFinish.  
- Finally, we found out that OpenPorshSF was lowering our ajusted R2 and was not correlated to the SalePrice (cf. correlation matrix), so running the model without this feature was better.  

```{r}
col_to_remove = c("Street", "Utilities", "RoofMatl", "Condition2","Exterior1st", "Exterior2nd", "Electrical", "SaleCondition", "BsmtFinType2", "Heating","RoofStyle", "GarageCond", "ExterQual", "GarageFinish", "OpenPorchSF", "HeatingQC", "Functional")
for (name in col_to_remove){cat(which(colnames(home)==name)," ")}
```

```{r, include=FALSE}
home_streamlined = home[-c(5, 8, 21, 13, 22, 23, 40, 66, 34, 37, 20, 59, 26, 55, 62, 38, 51, 17 )]
```


```{r, include=FALSE}
full_model = lm(log(SalePrice)~., data = home_streamlined)
summary(full_model)
```
Running a model with all the variables excluding the ones we just removed, we obtain an adjusted R2 of 0.91. 

According to the T test above, the variables having the most impact to explain SalePrice are the ones describing: 
- the location of the home (e.g. MSZoning, Neighborhood)
- the area of the home (LotArea)
- overall quality and condition ( OverallQual, OverallCond)
- # Comment/reword the specificities of some aspects of the home such as the roof (e.g. RoofStyleShed, RoofMat) and security functions (e.g. Fireplaces). 
- the construction period (e.g. YearBuilt, YearRemodAdd)

Many variables could be removed from our model while marginally affecting its efficiency to explain SalePrice. To improve model efficiency, we will perform a selection of variables based on forward, backward and both methods. 
  
```{r, message= FALSE, comment=FALSE}
# Reducing our number of features using the 3 selection methods seen in class (forward, backward, both)
library(MASS)
# Forward method :
linear_select_variables_forward = stepAIC(full_model, data=home_streamlined, trace=0, direction=c("forward"), verbose = FALSE)
#summary(linear_select_variables_forward)

# Backward method
linear_select_variables_backward = stepAIC(full_model,~., trace=0, direction=c("backward") )
#summary(linear_select_variables_backward)

# Both
linear_select_variables_both = stepAIC(full_model, trace=0, direction=c("both"))
#summary(linear_select_variables_both)
```

To choose among the three methods, we retrieve the AIC of each model and choose the one with the smallest AIC.  
```{r}
extractAIC(linear_select_variables_forward)
extractAIC(linear_select_variables_backward)
extractAIC(linear_select_variables_both)
```
  
We choose the model extracted by the backward or the both method as they have the same AIC, smaller than the one of the forward method. We choose arbitrarily the backward one. For this specific model, let's verify that the postulates are verified.   
  
```{r}
# Validate (or not) the postulates 
par(mfrow=c(3,3))
plot(linear_select_variables_backward, which=1) # P1
plot(linear_select_variables_backward, which=3) # P2
acf(residuals(linear_select_variables_backward), main= "Auto-correlation plot") # P3
plot(linear_select_variables_backward, which=2) # P4
plot(linear_select_variables_backward, which=5) # Cook
```
Looking at the graphs, the postulates are verified for this model.  
  
Now, we want to verify that we don't have outliers in our model. 
```{r}
library(car)
influenceIndexPlot(linear_select_variables_backward)
```
Cook's distance plot: according to the Cook's criteria, we don't observe any leverage point or regression outlier.  
Studentized residuals plot: according to the plot, there are a lot of outliers (many points below -2), which is confusing. The two main outliers according to this criteria oare 524 and 1299. 
Bonferroni's plot : we notice that 13 points have a p-value inferior to 0.5, so they are outliers according to this criteria.  
Hat plot: two points (327 and 783) seem to be leverage points according to this criteria.  
  
Based on those results, we decided to run an outlier test for more precision: 
```{r}
# Finding the outliers of our model
outlierTest(linear_select_variables_backward)
```
Using this test, 7 possible outliers are given according to the studentized criterion and Bonferroni p-value. We can now test our model without them.
Note that those tests are based on confidence intervals and then for this reason, given the number of observations, we could have up to 70 outliers. 
  
```{r}
# Plotting our regression outliers
home_streamlined[c(199, 596, 336, 692, 633, 618, 323,809),1:ncol(home_streamlined)]
median(home_streamlined$SalePrice)
```

```{r}
# Building the dataframe without those outliers 
home_no_outliers = home_streamlined[-c(199, 596, 336, 692, 633, 618, 323,809), ]
```

```{r}
# Displaying our regression model to use it on our new dataframe without outliers
#linear_select_variables_backward
```


```{r}
# Building the same regression model as before with our new dataframe 
linear_backward_without_outliers <- lm(formula = log(SalePrice) ~ MSSubClass + MSZoning + LotFrontage + 
    LotArea + LandContour + LotConfig + LandSlope + Neighborhood + 
    Condition1 + OverallQual + YearBuilt + YearRemodAdd + MasVnrType + 
    MasVnrArea + ExterCond + Foundation + BsmtCond + BsmtExposure + 
    BsmtFinSF1 + BsmtUnfSF + CentralAir + GrLivArea + BsmtFullBath + 
    FullBath + HalfBath + KitchenQual + Fireplaces + GarageType + 
    GarageCars + GarageQual + WoodDeckSF + MoSold + YrSold, data = home_streamlined)
```

```{r}
# Looking at the results
summary(linear_backward_without_outliers)
```

```{r, message= FALSE, comment=FALSE}
# Testing the postulates on our new model 
par(mfrow=c(3,3))
plot(linear_backward_without_outliers, which=1) # P1
plot(linear_backward_without_outliers, which=3) # P2
acf(residuals(linear_backward_without_outliers), main= "Auto-correlation plot") # P3
plot(linear_backward_without_outliers, which=2) # P4
plot(linear_backward_without_outliers, which=5) # outliers wusing cook's distance
```

According to the graphs, P1 and P3 are validated without hesitation. 
For P2, we notice an slight elliptic behaviour in the middle, but it seems slight enough to validate the postulate. 
For P4, we have tail observations that do not fit the normal distribution. Based on the high number of observations in our dataframe and the few number of points not aligned with the normal distribution quantiles, we validate the postulate. 

\textbf{IV. Final models}  
  
Parameters of our model:  
Error:  

```{r, include=FALSE}
test = test[,2:ncol(test)]
# Reaffecting test's features to factors
test$OverallQual = as.factor(test$OverallQual)
test$OverallCond = as.factor(test$OverallCond)
test$MoSold = as.factor(test$MoSold)
test$MSSubClass = as.factor(test$MSSubClass)
test$Fireplaces = as.factor(test$Fireplaces)

# Regrouping neighbourhoods of less than 20 and 50 sales together
levels(test$Neighborhood) <- c(levels(test$Neighborhood), "N_Under20Sales","N_Under50Sales") 
test$Neighborhood[test$Neighborhood %in% c("Blueste","NPkVill","Veenker","BrDale","Blmngtn","MeadowV")] = "N_Under20Sales"
test$Neighborhood[test$Neighborhood %in% c("StoneBr","SWISU","ClearCr","IDOTRR","Timber","NoRidge","Mitchel")] = "N_Under50Sales"

# Roofstyle
levels(test$RoofStyle) <- c(levels(test$RoofStyle), "RS_Other") 
test$RoofStyle[test$RoofStyle %in% c("Shed","Mansard","Gambrel","Flat")] = "RS_Other"

# Condition 1
levels(test$Condition1) <- c(levels(test$Condition1), "C_Other") 
test$Condition1[test$Condition1 %in% c("RRNe","RRNn","PosA","RRAe","PosN","RRAn","Artery")] = "C_Other"

# ExterCond
levels(test$ExterCond) <- c(levels(test$ExterCond), "EC_Other") 
test$ExterCond[test$ExterCond %in% c("Po", "Ex", "Fa", "Gd")] = "EC_Other"

# OverallQual
levels(test$OverallQual) <- c(levels(test$OverallQual), "Very_Low") 
test$OverallQual[test$OverallQual %in% c("-4.27238846180047", "-3.28279659466729", "-2.38962301582022")] = "Very_Low"
```

```{r}
head(test)
```

```{r}
# Test error
mean((log(test$SalePrice) - predict.lm(linear_backward_without_outliers, test)) ^ 2)
```

Confidence interval:
```{r}
newgrille = data.frame(LotArea=seq(min(home_no_outliers$LotArea)+1,max(home_no_outliers$LotArea)-1),2) 
predicgrille = predict.lm(linear_backward_without_outliers, newgrille, interval='confidence',level=0.95) 
plot(home_no_outliers$LotArea,home_no_outliers$SalePrice,ylab="SalePrice",xlab="LotArea",pch=20)
#abline(linear_backward_without_outliers,col='red')
matlines(newgrille$SalePrice,predicgrille,lty=c(1,2,2),col=c("red","blue","blue")) 
predicgrille = predict.lm(linear_backward_without_outliers,newgrille, interval='prediction',level=0.95) 
matlines(newgrille$SalePrice,predicgrille,lty=c(1,2,2),col=c("red","green","green"))
```
```{r}
# Assuming gaussianity 
summary(linear_backward_without_outliers)$coefficients
```



```{r}
p-value:  
Estimate of the generalization error of our final model:  training data 
  
\textbf{V. Discussion}  
Final conclusions on our model:  
key learnings:  
Improvements for future projects: