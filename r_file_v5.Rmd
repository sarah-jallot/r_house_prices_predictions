---
title: "Regression - Final project"
author: "Victoire de Termont and Sarah Jallot"
date: "12/3/2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
```
Our objective is to predict the prices of residential homes in Ames, Iowa accurately by exploiting an existing dataset to its best. Our key metric will be the MSE on test data.     
The training data consists of 66 categorical and quantitative variables extensively describing 1,095 residential homes in Ames, Iowa, and the houses' corresponding price in dollars. Discriminating between relevant and non relevant regressors to foster sparsity in our model is paramount for an efficient and robust model.  
    
To achieve this, we first investigate numerical and factor variables sequentially. Applying the log to SalePrice likens it to a Gaussian distribution, with a few outliers. A PCA analysis on numerical variables showed us that 23/28 numerical variables accounted for 99% of the sample variance. Factorwise, we distinguish three types of regressors: factors with an overrepresented level, some with high cardinality, and others with standard level repartition. 
  
We then perform elementary factor pruning and intra-factor modality regrouping before using ANOVA to remove factors manually. We do not touch numerical variables but instead rely on our model to make a selection.  
  
We choose to fit two linear models using stepwise selection procedure with the AIC criterion to foster sparsity. In the first model, the only modification we impose on the output is applying the logarithm, before removing outliers after testing them. In the second model, we apply a winsorisation method to log(SalePrice) as we noticed that most outliers located to the LHS of price made our model less robust.  In both cases, our best model according to AIC is the backward selection model. Model 1 predicts the test data much more accurately than model 2 (24 000$ vs 33 000$), but model 2 validates the regression assumptions better. XXXX what do we predict better?? We choose to retain model 1 as we want our model to generalise well, and model 1 is satisfactory enough on the postulates. 
Our final model has an MSE of XX.  
  
    

\textbf{I. Data description} 
We first explore the pre-processed data to get intuition of an efficient model. Understanding and preprocessing the data implies that we understand what we are given, check that there are no missing values, and that R correctly categorises each regressor. 

```{r, include = FALSE}
load("DataProject.RData")
home = train[,2:ncol(train)]
head(home)
```
The data we are handling is heterogeneous, although it is presented in a structured manner. We are dealing both with categorical and quantitative variables, which we will consider separately.  

`````{r, include = FALSE } 
summary(home)
```
We observe in the data summary that a full model would include 66 features and an intercept, and 1,095 observations.  
The list of features is extensive: not all regressors will be useful to predict house price. For instance, we expect the variable MSZoning to have much more impact on the price that the variable Heating, as the heating system is something that can be changed, whereas the location is permanent.  
Quantitative variables differ in scale and range: prices start from ~35,000 dollars, and can attain 755,000. Before pre-processing, surfaces took higher values than bedrooms above ground which ranged from 1 to 8. Scaling the data allows to harmonise it, so we keep the scaling. 
    
```{r, include = FALSE}
str(home, give.attr = FALSE)
```
R appears to cast some factors as integers: mainly areas and ratings. We decide not to consider years as factors as we want our predictions to generalise to other, unencountered years. We recast OverallQual, OverallCond, MoSold, MSSubClass and Fireplaces as factors.  
All quantitative features are integers and price, the output, is the only numeric. We know that R automatically encodes factors, and we choose to keep the by default levels, which are  alphabetically ordered. 

```{r}
home$OverallQual = as.factor(home$OverallQual)
home$OverallCond = as.factor(home$OverallCond)
home$MoSold = as.factor(home$MoSold)
home$MSSubClass = as.factor(home$MSSubClass)
home$Fireplaces = as.factor(home$Fireplaces)
```
  
We check there are no missing values in the pre-processed data before launching into the analysis.  
```{r}
cat("There are", sum(is.na(home)), "missing values in our dataframe.")
```
  
\textbf{II. Data exploration} 
  
1. Numerical analysis.  
We describe the output data before analysing the correlation matrix. A PCA analysis we performed showed us that 23/28 numerical variables accounted for 99% of the variance. We left it out for concision. 
```{r}
# Extracting all numerical variables
nums = unlist(lapply(home, is.numeric))
home_numerical = (home[ , nums])
```
We first set out to observe the output, SalePrice. 
```{r, fig.width = 5, fig.height = 4, fig.align = "center"}
par(mfrow=c(1,2))
# A few observations on SalePrice. 
hist(home_numerical$SalePrice, main = "SalePrice", cex.main = 0.8, cex.axis = 0.5, cex.lab = 0.5)
boxplot(home_numerical$SalePrice, main = "SalePrice", cex.main = 0.8, cex.axis = 0.5, cex.lab = 0.5)
```

```{r}
library(e1071) 
cat("Data skewness is", skewness(home$SalePrice))
```
SalePrice is highly skewed to the right : we confirm this by noting that Saleprice mean is ~181,196 whereas the median is much lower at ~164,500. 
SalePrice is volatile with many houses to the left hand side, but with a number of outliers to the right with extreme values. To smoothen the output and approach a normal distribution, we will consider the log when fitting our model. If this isn't enough, we could go a step further by either trimming or modifying outlier values to improve our generalisation error.

```{r, fig.width = 5, fig.height = 3, fig.align = "center"}
par(mfrow=c(1,3))
hist(log(home$SalePrice), cex.main = 1.2, main = "log(SalePrice)")
boxplot(log(home_numerical[,"SalePrice"]),cex.main = 1.2)
qqnorm(log(home_numerical$SalePrice),cex.main = 1.2)
```
log(SalePrice) is pretty close to a normal distribution, except for extreme values. 

```{r, fig.width = 3, fig.height = 3, fig.align = "center"}
# Correlation matrix for numerical features
library(corrplot)
R = round(cor(home_numerical),2)
corrplot(R, method='ellipse', tl.cex = 0.5)
```
The numerical features that are the most correlated with SalePrice: GrLivArea, GarageArea and GarageCars, 1st & 2ndFlrSF, YearBuilt GarageYrBuilt & YearRemodAdd. Areas and surfaces are all related to square feet, which we know is a key driver in house sales. The construction or modernisation works are an indicator of overall quality of the housing and the investments that went into it, so it makes sense for them to be correlated to SalePrice. 
On the contrary, YrSold and BsmtHalfBath are poorly correlated to SalePrice. YrSold is correlated to none of the other features, so it is an irrelevant regressor: the decision to sell a house doesn't have much to do with what drives its price or the price one can sell it at. 
The correlation matrix does not take into account feature interactions, so we will leave numerical feature selection to our stepwise model procedures. 

Our intuition is that three feature types mainly drive SalePrices: area/surface, location and quality. 
Let's describe 1stFlrSF as it is the closest feature to square feet with 2ndFloorSF.
```{r, fig.width = 6, fig.height = 4, fig.align = "center"}
par(mfrow=c(1,3))
# Surface
hist((home_numerical$`1stFlrSF`), main = "1stFloorSF", cex.main = 0.8)
boxplot((home_numerical$`1stFlrSF`), main = "1stFloorSF", cex.main = 0.8)
qqnorm(home_numerical$`1stFlrSF`,cex.main = 1.2)
```
From these three graphs, we can assume a Gaussian distribution on 1stFloorSF. 

## PCA 
Because PCA works best with (scaled) numerical data, we will perform it on our scaled numerical feature columns.
```{r}
home_numerical_output = home_numerical$SalePrice
home_numerical_features = home_numerical[,2:ncol(home_numerical)] # we will perform the PCA analysis on this dataset. 

pca.train = home_numerical_features
prin_comp = prcomp(pca.train, center = TRUE, scale. = TRUE)

# Compute standard deviation of each principal component
std_dev = prin_comp$sdev

# Compute variance
pr_var = std_dev^2
prop_varex = pr_var/sum(pr_var)

# Cumulative scree plot
plot(cumsum(prop_varex), 
     xlab = "Principal Component", 
     ylab = "Cumulative Proportion of Variance Explained",
     type = "b")

abline(h=1, col='red')
abline(v=23, col='black', lty = 3)
```
23 of our 28 numerical features account for 99% of the variance.  
  

3. Factor analysis  
We first investigate level fragmentation within the factors. We discover three types of factors, of which examples are given below.  
```{r, fig.width = 5, fig.height = 3, fig.align = "center"}
par(mfrow=c(1,3))
# Getting our factors
factors = unlist(lapply(home, is.factor))
home_factors = home[,factors]

# Street 
barplot(table(home_factors$Street)[order(table(home_factors$Street))], main = "Street")
# Neighbourhood
barplot(table(home_factors$Neighborhood)[order(table(home_factors$Neighborhood))], main = "Neighbourhood")
# Garage Finish
barplot(table(home_factors$GarageFinish)[order(table(home_factors$GarageFinish))], main = "GarageFinish")
```
i) Clear surrepresentation of one level versus the others. 
In Street and Utilities, which are binary, this is conspicuous. From this we infer that these factors won't be very useful in predicting house price in general: nearly all houses will be in the same category along these factors (and for those who aren't, data is too sparse to generalise well).  
Note that this is also the case for RoofMatl, Heating, BsmtFinType2, Electrical, GarageCond, GarageQual.  
  
ii) High cardinality in the number of levels: this is especially the case for neighbourhood, which has 25 levels. We regroup some of these levels together to improve our predictions.
Note that this is also the case for Exterior1st, Exterior2nd, BsmtExposure. 
  
iii) Classic factor repartition with reasonable representation of each modality, as is the case for Housestyle for instance. 
Note that this is also the case for HeatingQC, GarageFinish, BsmtFinType1.  

Intuitively, we said that both location and overall quality will impact SalePrice significantly. Let us check this with anova and ancova tests. 
```{r, fig.height = 3, fig.align = "center"}
library(gridExtra)
# Area
par(mfrow=c(1,2))
library(ggplot2)
plot1 = ggplot(home, 
       aes(x=MSZoning, 
           y=SalePrice, 
           colour=MSZoning, 
           group = MSZoning, 
           fill = MSZoning)) + geom_boxplot(alpha=0.5, outlier.alpha=0) +geom_jitter(width=0.1)

# Overall quality 
plot2 = ggplot(home, 
       aes(x=OverallQual, 
           y=SalePrice, 
           colour= OverallQual, 
           group = OverallQual, 
           fill = OverallQual)) + geom_boxplot(alpha=0.5, outlier.alpha=0) +geom_jitter(width=0.1)

grid.arrange(plot1,plot2,ncol=2)
```

It appears that houses from the RM and RH areas are less expensive than the ones from FV and RL areas.
```{r}
# Anova on area
mod1<-lm(SalePrice ~ MSZoning-1 , data=home)
anova(mod1)

# Anova on OverallQual
mod2<-lm(SalePrice ~ OverallQual-1 , data=home)
anova(mod2)
```

Anova tests seem to show that both area and overall quality have a strong effect on SalePrice. However, here we have not accounted for the interaction between MSzoning and overall quality: to validate our conclusions, we must show that it is not significant with an ANCOVA test. 

```{r}
# Ancova on quality and area
mod4 = lm(SalePrice~1 + MSZoning + OverallQual + OverallQual:MSZoning, data = home)
anova(mod4)
```
SalePrice highly depends on MSZoning, OverallQual and their interaction. So we will keep these factors.  

\textbf{III. Modeling and diagnostics}  
Given the number of regressors, we choose a model favouring feature sparsity. We choose to use a stepwise selection procedure with an AIC criterion. 
We will predict SalePrice's logarithm to improve our target smoothness and estimated residuals homoscedasticity. In initial models we ran, we noticed that our 8 outliers were located towards the extreme bottom values of SalePrice. They prevented us from validating our postulates, mainly homoscedasticity and gaussianity. Instead of removing them, we use the winsor method to reaffect extreme values and normalise our data. Doing this greatly improves model robustness (the postulates) while diminishing our MSE on test data
  
After manually removing poorly informative factors and other highly correlated numerical features, we start by fitting a full linear model to predict log(SalePrice).
Looking at our dataframe, we realised that some factor columns had a lot of differents modalities, with some that appeared very few times. Thus, for more efficiency of our algorithm, we decided to group them. We performed this for Neighbourhood, RoofStyle, Condition1, ExterCond and OverallQual
. 
```{r}
winsor1 = function (x, fraction=.05)
{if(length(fraction) != 1 || fraction < 0 ||fraction > 0.5) {stop("bad value for 'fraction'")}
lim <- quantile(x, probs=c(fraction, 1-fraction))
x[ x < lim[1] ] <- lim[1]
x[ x > lim[2] ] <- lim[2]
x}

home$SalePrice= (winsor1(home$SalePrice, fraction=.05))
```

We noted earlier that many categorical variables could be considered uninformative or redundant. 
- We removed factors with massively overrepresented categories: Street, Utilities, RoofMatl, Condition2, Heating, Electrical, Functional, GarageCond.  
We also made some regroupments within factors to diminish cardinality, mostly for neighbourhood. 
```{r, include = FALSE}
col_to_remove = c("Street", "Utilities", "RoofMatl", "Condition2","Heating", "Electrical", "Functional", "GarageCond","Exterior1st", "Exterior2nd", "OverallCond", "HeatingQC", "SaleCondition", "BsmtFinType2","RoofStyle", "ExterQual")
for (name in col_to_remove){cat(which(colnames(home)==name)," ")}
```

```{r, include=FALSE}
# Elementary pruning and regrouping of categories
# Regrouping neighbourhoods of less than 20 and 50 sales together 
table_neighborhood= table(home$Neighborhood)
table_neighborhood[order(table_neighborhood)]

levels(home$Neighborhood) <- c(levels(home$Neighborhood), "N_Under20Sales","N_Under50Sales") 
home$Neighborhood[home$Neighborhood %in% c("Blueste","NPkVill","Veenker","BrDale","Blmngtn","MeadowV")] = "N_Under20Sales"
home$Neighborhood[home$Neighborhood %in% c("StoneBr","SWISU","ClearCr","IDOTRR","Timber","NoRidge","Mitchel")] = "N_Under50Sales"
```

```{r, include=FALSE}
# Roofstyle
table(home$RoofStyle)[order(table(home$RoofStyle))]
levels(home$RoofStyle) <- c(levels(home$RoofStyle), "RS_Other") 
home$RoofStyle[home$RoofStyle %in% c("Shed","Mansard","Gambrel","Flat")] = "RS_Other"

# Condition 1
table(home$Condition1)[order(table(home$Condition1))]
levels(home$Condition1) <- c(levels(home$Condition1), "C_Other") 
home$Condition1[home$Condition1 %in% c("RRNe","RRNn","PosA","RRAe","PosN","RRAn","Artery")] = "C_Other"

# ExterCond
table(home$ExterCond)[order(table(home$ExterCond))]
levels(home$ExterCond) <- c(levels(home$ExterCond), "EC_Other") 
home$ExterCond[home$ExterCond %in% c("Po", "Ex", "Fa", "Gd")] = "EC_Other"

# OverallQual
table(home$OverallQual)
levels(home$OverallQual) <- c(levels(home$OverallQual), "Very_Low") 
home$OverallQual[home$OverallQual %in% c("-4.27238846180047", "-3.28279659466729", "-2.38962301582022")] = "Very_Low"
```

```{r, include=FALSE}
home_streamlined = home[-c(5, 8, 21, 13, 37, 40, 51, 59, 22, 23, 17, 38, 66, 34, 20, 26)]
```

- Based on Anova tests, we removed other factors to improve model robustness: OverallCond, Exterior1st, Exterior2nd. OverallCond in particular was too specific and weakened our model by creating observations with leverage one.
```{r}
# Ancova on OverallQual and OverallCond
mod5 = lm(SalePrice~1 + OverallQual + OverallCond + OverallQual:OverallCond, data = home)
anova(mod5)
```
- For some features, we were not sure wether or not they had an influence, so we tested the model with and without them and removed them if they were not improving our score : HeatingQC, SaleCondition, BsmtFinType2, RoofStyle, ExterQual. 


```{r, include = FALSE}
full_model = lm(log(SalePrice)~., data = home_streamlined)
summary(full_model)
```
Running a model with all the variables excluding the ones we just removed, we obtain an adjusted R2 of 0.91. 
Many variables could be removed from our model while marginally affecting its efficiency to explain SalePrice. To improve model efficiency, we perform a selection of variables based on forward, backward and both methods. 
```{r, include = FALSE}
# Reducing our number of features using the 3 selection methods seen in class (forward, backward, both)
library(MASS)
# Forward method :
linear_select_variables_forward = stepAIC(full_model, data=home_streamlined, trace=0, direction=c("forward"), verbose = FALSE)
summary(linear_select_variables_forward)

# Backward method
linear_select_variables_backward = stepAIC(full_model,~., trace=0, direction=c("backward") )
summary(linear_select_variables_backward)

# Both
linear_select_variables_both = stepAIC(full_model, trace=0, direction=c("both"))
summary(linear_select_variables_both)
```

To choose among the three methods, we retrieve the AIC of each model and choose the one with the smallest AIC.  
```{r}
extractAIC(linear_select_variables_forward)
extractAIC(linear_select_variables_backward)
extractAIC(linear_select_variables_both)
```
We choose the model extracted by the backward or the both method as they have the same AIC, smaller than the one of the forward method. We arbitrarily choose the backward model. For this specific model, let's verify that the postulates are verified.   
  
```{r}
# Validate (or not) the postulates 
par(mfrow=c(3,3))
plot(linear_select_variables_backward, which=1) # P1
plot(linear_select_variables_backward, which=3) # P2
acf(residuals(linear_select_variables_backward), main= "Auto-correlation plot") # P3
plot(linear_select_variables_backward, which=2) # P4
plot(linear_select_variables_backward, which=5) # Cook
```
Looking at the graphs, the postulates are verified for this model.  
Now, we want to verify that we don't have outliers in our model. 
```{r, fig.width = 4, fig.height = 3, fig.align = "center"}
library(car)
influenceIndexPlot(linear_select_variables_backward)
```
Cook's distance plot: according to the Cook's criteria, we don't observe any leverage point or regression outlier.  
Studentized residuals plot: according to the plot, there are a lot of outliers (many points below -2), which is confusing. The two main outliers according to this criteria oare 524 and 1299. 
Bonferroni's plot : we notice that 13 points have a p-value inferior to 0.5, so they are outliers according to this criteria.  
Hat plot: two points (327 and 783) seem to be leverage points according to this criteria.  
  
Based on those results, we decided to run an outlier test for more precision: 
```{r}
# Finding the outliers of our model
outlierTest(linear_select_variables_backward)
```
Using this test, 4 possible outliers are given according to the studentized criterion and Bonferroni p-value. We can now test our model without them.
Note that those tests are based on confidence intervals and then for this reason, given the number of observations, we could have up to 70 outliers. 

```{r}
# Plotting our regression outliers
home_streamlined[c(199, 336, 596, 633),1:ncol(home_streamlined)]
```

```{r, include = FALSE}
# Building the dataframe without those outliers 
home_no_outliers = home_streamlined[-c(199, 336, 596, 633), ]
```

```{r}
# Displaying our regression model to use it on our new dataframe without outliers
linear_select_variables_backward
```

```{r}
# Building the same regression model as before with our new dataframe 
linear_backward_without_outliers <- lm(formula = log(SalePrice) ~ MSSubClass + MSZoning + LotFrontage + 
    LotArea + LotConfig + Neighborhood + Condition1 + OverallQual + 
    YearBuilt + YearRemodAdd + MasVnrType + MasVnrArea + BsmtCond + 
    BsmtExposure + BsmtFinSF1 + BsmtUnfSF + TotalBsmtSF + CentralAir + 
    `1stFlrSF` + GrLivArea + BsmtFullBath + FullBath + HalfBath + 
    KitchenQual + Fireplaces + GarageType + GarageYrBlt + GarageCars + 
    GarageQual + WoodDeckSF + MoSold + YrSold, data = home_streamlined)
```

```{r}
# Looking at the results
#summary(linear_backward_without_outliers)
```

```{r, message= FALSE, comment=FALSE}
# Testing the postulates on our new model 
par(mfrow=c(3,3))
plot(linear_backward_without_outliers, which=1) # P1
plot(linear_backward_without_outliers, which=3) # P2
acf(residuals(linear_backward_without_outliers), main= "Auto-correlation plot") # P3
plot(linear_backward_without_outliers, which=2) # P4
plot(linear_backward_without_outliers, which=5) # outliers wusing cook's distance
```
According to the graphs, P1 and P3 are validated without hesitation. 
For P2, we notice an slight elliptic behaviour in the middle, but it seems slight enough to validate the postulate. 
For P4, we have tail observations that do not fit the normal distribution. Based on the high number of observations in our dataframe and the few number of points not aligned with the normal distribution quantiles, we validate the postulate. 

\textbf{IV. Final models}  
  
Parameters of our model:  
Finally, our final model without outliers is the one we built reaffecting some features, regrouping some categories for factors and retreving repretitive or non significant variables. Then, we used a backward selection method to reduce the number of features and keep only the necessary ones. Our model had 
Our final model had 7 outliers that we decided to remove after outlier tests. 
```{r}
#linear_backward_without_outliers
```
It can be written: lm(formula = log(SalePrice) ~ MSSubClass + MSZoning + LotFrontage + 
    LotArea + LotConfig + Neighborhood + Condition1 + OverallQual + 
    YearBuilt + YearRemodAdd + MasVnrType + MasVnrArea + BsmtCond + 
    BsmtExposure + BsmtFinSF1 + BsmtUnfSF + TotalBsmtSF + CentralAir + 
    `1stFlrSF` + GrLivArea + BsmtFullBath + FullBath + HalfBath + 
    KitchenQual + Fireplaces + GarageType + GarageYrBlt + GarageCars + 
    GarageQual + WoodDeckSF + MoSold + YrSold, data = home_no_outliers)

Looking at the coefficients, the most impactful features are MSZoning, OverallQual and Fireplaces by far, which is not suprising. Indeed, we stated in the beginning that the location was key. Also, the quality is very important because if conditions are not satisfied, then the flat would require some work, which cost would be deducted from the final price. Finally, having a fireplace is a good indicator of the standard of a home, as they tend to be built in expensive flats.
```{r, include=FALSE}
# Testing our model on the test set 
test = test[,2:ncol(test)]
# Reaffecting test's features to factors
test$OverallQual = as.factor(test$OverallQual)
test$OverallCond = as.factor(test$OverallCond)
test$MoSold = as.factor(test$MoSold)
test$MSSubClass = as.factor(test$MSSubClass)

# Regrouping neighbourhoods of less than 20 and 50 sales together
levels(test$Neighborhood) <- c(levels(test$Neighborhood), "N_Under20Sales","N_Under50Sales") 
test$Neighborhood[test$Neighborhood %in% c("Blueste","NPkVill","Veenker","BrDale","Blmngtn","MeadowV")] = "N_Under20Sales"
test$Neighborhood[test$Neighborhood %in% c("StoneBr","SWISU","ClearCr","IDOTRR","Timber","NoRidge","Mitchel")] = "N_Under50Sales"

# Roofstyle
levels(test$RoofStyle) <- c(levels(test$RoofStyle), "RS_Other") 
test$RoofStyle[test$RoofStyle %in% c("Shed","Mansard","Gambrel","Flat")] = "RS_Other"

# Condition 1
levels(test$Condition1) <- c(levels(test$Condition1), "C_Other") 
test$Condition1[test$Condition1 %in% c("RRNe","RRNn","PosA","RRAe","PosN","RRAn","Artery")] = "C_Other"

# ExterCond
levels(test$ExterCond) <- c(levels(test$ExterCond), "EC_Other") 
test$ExterCond[test$ExterCond %in% c("Po", "Ex", "Fa", "Gd")] = "EC_Other"

# OverallQual
levels(test$OverallQual) <- c(levels(test$OverallQual), "Very_Low") 
test$OverallQual[test$OverallQual %in% c("-4.27238846180047", "-3.28279659466729", "-2.38962301582022")] = "Very_Low"
```

```{r}
# We look for the average percentage of SalePrice that we fail to predict
sqrt(mean((test$SalePrice - exp(predict.lm(linear_backward_without_outliers, test))) ^ 2))
```

```{r}
residuals = test$SalePrice - exp(predict.lm(linear_backward_without_outliers, test))
residuals_percent = abs(residuals/test$SalePrice)
sqrt(mean(residuals_percent))
```
```{r}
sqrt(mean(log(test$SalePrice)-predict.lm(linear_backward_without_outliers, test))^2)
```


```{r}
# RMSE
sqrt(mean((test$SalePrice - exp(predict.lm(linear_backward_without_outliers, test))) ^ 2))
```

```{r}
# Average price
# mean(test$SalePrice)
```


Testing our regression model on the test set, we fail to predict around 13% of the sale price, which corresponds to a RMSE of ~24k, for homes with an average price avec ~180k. This seems acceptable for the price of a home.  
We obtain a p-value that is below 2.2e-16, which is much less than 0.05. We reject the null hypothesis that the intercept only is better to explain our SalePrice than the model we have built.  

Confidence interval: as our regression model is mostly impacted by factors, doing confidence intervals does not help us visualize the proximity among SalePrice and the most important features. However, doing a 95% confidence interval with LotArea, we notice that this variable is not good at emplaning SalePrice by itself, as the price can double for a same value of LotArea. 
```{r, fig.width=5, fig.height=3, fig.align="center"}
model <- lm(SalePrice ~ LotArea, data=home)
plot(home$LotArea, home$SalePrice, xlab="LotArea", ylab="SalePrice", main="Regression", cex.main=0.8, cex.axis=0.5, cex.lab=0.5)
abline(model, col="lightblue")

newx <- seq(0, 7, by=0.05)
pred_interval <- predict(model, newdata=data.frame(LotArea=newx), interval="prediction",
                         level = 0.95)
lines(newx, pred_interval[,2], col="orange", lty=2)
lines(newx, pred_interval[,3], col="orange", lty=2)
```


\textbf{V. Discussion}  
  
Final conclusions on our model:
In the end, we improved a lot the model that we have compared to the one we had when running a linear regression on the original dataframe, as our postulates are now better validated with twice as less features.  
  
This project enabled us to apply all the methods we saw in class to analyze a given dataframe and find an acceptable manner to transform it into a more efficient dataframe. Working in pair was very positive, because it allowed us to get more ideas, and to share our understanding of the functions. We could be more efficient next time now that we have gained insights into the R manner to solve a regression problem.  
To go further, we could have used a lasso regression because we still have around 30 features. We tried to do so but had struggles plotting our postulates afterwards, and therefore could not conclude. However, we will learn how to do it as it could have been very useful.  


\textbf{V. Discussion}  
  
Final conclusions on our model:
In the end, we improved a lot the model that we have compared to the one we had when running a linear regression on the original dataframe, as our postulates are now better validated with twice as less features.  
  
This project enabled us to apply all the methods we saw in class to analyze a given dataframe and find an acceptable manner to transform it into a more efficient dataframe. Working in pair was very positive, because it allowed us to get more ideas, and to share our understanding of the functions. We could be more efficient next time now that we have gained insights into the R manner to solve a regression problem.  
To go further, we could have used a lasso regression because we still have around 30 features. We tried to do so but had struggles plotting our postulates afterwards, and therefore could not conclude. However, we will learn how to do it as it could have been very useful.
Key learnings:  
Improvements for future projects: