---
title: "Regression - Final project"
author: "Victoire de Termont and Sarah Jallot"
date: "12/3/2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

\textbf{I. Introduction} 
```{r, include = FALSE}
load("DataProject.RData")
```
Our objective is to predict the prices of residential homes in Ames, Iowa accurately by exploiting an existing dataset to its best. Our key metric will be the RMSE on test data.     
The training data consists of 66 categorical and quantitative variables extensively describing 1,095 residential homes in Ames, Iowa, and the houses' corresponding price in dollars. 
We will preprocess and explore our data smartly, select the best performing features, and fit the most adapted regression model after validating the key assumptions needed to implement it.     

We will first explore the pre-processed data to get intuition of an efficient model. Understanding and preprocessing the data implies that we understand what we are given, check that there are no missing values, and that R correctly categorises each regressor.  
```{r, include = FALSE}
# Loading the preprocessed dataset 
home = train[,2:ncol(train)]
#head(home)
```
The data we are handling is heterogeneous, although it is presented in a structured manner. We are dealing both with categorical and quantitative variables, which we will have to preprocess separately.  

`````{r, include = FALSE } 
summary(home)
```
Displaying the summary of our dataframe, we see that a full model would include 66 features and an intercept, and 1,095 observations.  
The list of features is extensive: not all regressors will be useful to predict house price. For instance, the variable MSZoning is expected to have much more impact on the price that the variable Heating, as the heating system is something that can be changed, whereas the location is permanent. We will perform different types of analysis to keep only the most relevant variables in our model.
  
Quantitative variables differ in scale and range: prices start from ~35,000 dollars, and can attain 755,000, whereas bedrooms above ground range from 1 to 8 only for instance.  
This means that scaling our data could optimise model performance. 

```{r, include = FALSE}
str(home)
```
R appears to cast some factors as integers: mainly areas and ratings. We decide not to consider years as factors as we want our predictions to generalise to other, unencountered years. We recast OverallQual, OverallCond, MoSold, MSSubClass and Fireplaces as factors.  
  
All quantitative features are integers and price, the output, is the only numeric. We know that R automatically encodes factors, and we choose to keep the by default levels, which are  alphabetically ordered. 
```{r}
home$OverallQual = as.factor(home$OverallQual)
home$OverallCond = as.factor(home$OverallCond)
home$MoSold = as.factor(home$MoSold)
home$MSSubClass = as.factor(home$MSSubClass)
home$Fireplaces = as.factor(home$Fireplaces)
# head(home)
```

We check there are no missing values in the pre-processed data before launching into the analysis.  
```{r}
cat("There are", sum(is.na(home)), "missing values in our dataframe.")
```
\textbf{II. Exploratory data analysis / initial modeling} 

1. Numerical analysis  
We will extract trends and relationships among the regressors. Comparing them is also essential, for instance using pairplots.
```{r}
# Extracting all numerical variables
nums = unlist(lapply(home, is.numeric))
home_numerical = (home[ , nums])
```

We first set out to observe the output, SalePrice. 
```{r}
par(mfrow=c(1,2))
# A few observations on SalePrice. 
hist(home_numerical$SalePrice, main = "SalePrice")
boxplot(home_numerical$SalePrice, main = "SalePrice")
```

```{r}
library(e1071) 
cat("Data skewness is", skewness(home$SalePrice))
```
SalePrice is highly skewed to the right : we confirm this by noting that Saleprice mean is ~181,196 whereas the median is much lower at ~164,500. 
SalePrice is volatile with many houses to the left hand side, but with a number of outliers to the right with extreme values. To smoothen the output and approach a normal distribution, we will consider the log when fitting our model. If this isn't enough, we might have to go a step further by either trimming or modifying outlier values to improve our generalisation error.

```{r}
par(mfrow=c(1,3))
hist(log(home$SalePrice))
boxplot(log(home_numerical[,"SalePrice"]))
qqnorm(log(home_numerical$SalePrice))
```
log(SalePrice) is pretty close to a normal distribution, except for extreme values. 

```{r}
# Correlation matrix for numerical features
library(corrplot)
R = round(cor(home_numerical),2)
corrplot(R, method='ellipse')
```
Our intuition is that three feature types mainly drive SalePrices: location, area/surface and quality. Let's start by analysing the numerical variable LotArea.  
```{r}
par(mfrow=c(1,2))
# Area
hist((home_numerical$LotArea), main = "LotArea")
boxplot((home_numerical$LotArea), main = "LotArea")
```
We believe that areas, particularly LotArea, will be important when predicting SalePrice. 
Most areas are located within [-2,2] in our pre-processed dataset, with many outliers towards the left- or the right-hand-side. 

The numerical features that are the most correlated with SalePrice are OverallQual, TotalBsmtSE, X1stElrSF, GrLivArea, GarageCars and GarageArea. On the contrary, MoSold, YrSold and BsmtHalfBath are poorly correlated to SalePrice. As the correlation matrix does not take into account interactions between our features in predicting sales price, we will not infer from this which variables we will keep in our final model.
  
3. Factor analysis  
For our first contact with the factors, let us get a closer look at level repartition to get an idea of fragmentation within the factors.  
```{r}
par(mfrow=c(1,3))
# Getting our factors
factors = unlist(lapply(home, is.factor))
home_factors = home[,factors]

# Street 
barplot(table(home_factors$Street)[order(table(home_factors$Street))], main = "Street")
# Neighbourhood
barplot(table(home_factors$Neighborhood)[order(table(home_factors$Neighborhood))], main = "Neighbourhood")
# Garage Finish
barplot(table(home_factors$GarageFinish)[order(table(home_factors$GarageFinish))], main = "GarageFinish")
```
Observing the factor columns, we see three types of repartitions.  
i) Clear surrepresentation of one level versus the others. 
In Street and Utilities, which are binary, this is conspicuous. From this we infer that these factors won't be very useful in predicting house price in general: nearly all houses will be in the same category along these factors (and for those who aren't, data is too sparse to generalise well).  
Note that this is also the case for RoofMatl, Heating, BsmtFinType2, Electrical, GarageCond, GarageQual.  
  
ii) High cardinality in the number of levels: this is especially the case for neighbourhood, which has 25 levels. We regroup some of these levels together to improve our predictions.
Note that this is also the case for Exterior1st, Exterior2nd, BsmtExposure. 
  
iii) Classic factor repartition with reasonable representation of each modality, as is the case for Housestyle for instance. 
Note that this is also the case for HeatingQC, GarageFinish, BsmtFinType1.  

Intuitively, we said that both location and overall quality will impact SalePrice significantly. Let us check this with anova and ancova tests. 
```{r}
# Area
par(mfrow=c(1,2))
library(ggplot2)
ggplot(home, 
       aes(x=MSZoning, 
           y=SalePrice, 
           colour=MSZoning, 
           group = MSZoning, 
           fill = MSZoning)) + geom_boxplot(alpha=0.5, outlier.alpha=0) +geom_jitter(width=0.1)
```


```{r}
# Overall quality 
ggplot(home, 
       aes(x=OverallQual, 
           y=SalePrice, 
           colour= OverallQual, 
           group = OverallQual, 
           fill = OverallQual)) + geom_boxplot(alpha=0.5, outlier.alpha=0) +geom_jitter(width=0.1)
```


```{r}
# Overall condition
ggplot(home, 
       aes(x=OverallCond, 
           y=SalePrice, 
           colour= OverallCond, 
           group = OverallCond, 
           fill = OverallCond)) + geom_boxplot(alpha=0.5, outlier.alpha=0) +geom_jitter(width=0.1)
```

It appears that houses from the RM and RH areas are less expensive than the ones from FV and RL areas.
```{r}
# Anova on area
mod1<-lm(SalePrice ~ MSZoning-1 , data=home)
anova(mod1)

# Anova on OverallQual
mod2<-lm(SalePrice ~ OverallQual-1 , data=home)
anova(mod2)
```

Anova tests seem to show that both area and overall quality have a strong effect on SalePrice. However, here we have not accounted for the interaction between MSzoning and overall quality: to validate our conclusions, we must show that it is not significant with an ANCOVA test. 

```{r}
# Ancova on quality and area
mod4 = lm(SalePrice~1 + MSZoning + OverallQual + OverallQual:MSZoning, data = home)
anova(mod4)
```
SalePrice highly depends on MSZoning, OverallQual and their interaction. So we will keep these factors.  


\textbf{III. Modeling and diagnostics}  
Given the number of regressors, we choose a model favouring feature sparsity. We choose to use a stepwise selection procedure with an AIC criterion. 
  
We will predict SalePrice's logarithm to improve our target smoothness and estimated residuals homoscedasticity. In initial models we ran, we noticed that our 8 outliers were located towards the extreme bottom values of SalePrice. They prevented us from validating our postulates, mainly homoscedasticity and gaussianity. Instead of removing them, we use the winsor method to reaffect extreme values and normalise our data. Doing this greatly improves model robustness (the postulates) while diminishing our MSE on test data. 
```{r}
winsor1 = function (x, fraction=.05)
{if(length(fraction) != 1 || fraction < 0 ||fraction > 0.5) {stop("bad value for 'fraction'")}
lim <- quantile(x, probs=c(fraction, 1-fraction))
x[ x < lim[1] ] <- lim[1]
x[ x > lim[2] ] <- lim[2]
x}

home$SalePrice= (winsor1(home$SalePrice, fraction=.05))
```

We noted earlier that many categorical variables could be considered uninformative or redundant. 
- We removed factors with massively overrepresented categories: Street, Utilities, RoofMatl, Condition2, Heating, Electrical, Functional, GarageCond.  
```{r, include = FALSE}
col_to_remove = c("Street", "Utilities", "RoofMatl", "Condition2","Heating", "Electrical", "Functional", "GarageCond","Exterior1st", "Exterior2nd", "OverallCond", "HeatingQC", "SaleCondition", "BsmtFinType2","RoofStyle", "ExterQual")
for (name in col_to_remove){cat(which(colnames(home)==name)," ")}
```

```{r, include=FALSE}
home_streamlined = home[-c(5, 8, 21, 13, 37, 40, 51, 59, 22, 23, 17, 38, 66, 34, 20, 26)]
```

- Based on Anova tests, we removed other factors to improve model robustness: OverallCond, Exterior1st, Exterior2nd. OverallCond in particular was too specific and weakened our model by creating observations with leverage one.
```{r}
# Ancova on OverallQual and OverallCond
mod5 = lm(SalePrice~1 + OverallQual + OverallCond + OverallQual:OverallCond, data = home)
anova(mod5)
```
- For some features, we were not sure wether or not they had an influence, so we tested the model with and without them and removed them if they were not improving our score : HeatingQC, SaleCondition, BsmtFinType2, RoofStyle, ExterQual. 

```{r, include=FALSE}
# Elementary pruning and regrouping of categories
# Regrouping neighbourhoods of less than 20 and 50 sales together 
table_neighborhood= table(home$Neighborhood)
table_neighborhood[order(table_neighborhood)]

levels(home$Neighborhood) <- c(levels(home$Neighborhood), "N_Under20Sales","N_Under50Sales") 
home$Neighborhood[home$Neighborhood %in% c("Blueste","NPkVill","Veenker","BrDale","Blmngtn","MeadowV")] = "N_Under20Sales"
home$Neighborhood[home$Neighborhood %in% c("StoneBr","SWISU","ClearCr","IDOTRR","Timber","NoRidge","Mitchel")] = "N_Under50Sales"
```

```{r, include=FALSE}
# Roofstyle
table(home$RoofStyle)[order(table(home$RoofStyle))]
levels(home$RoofStyle) <- c(levels(home$RoofStyle), "RS_Other") 
home$RoofStyle[home$RoofStyle %in% c("Shed","Mansard","Gambrel","Flat")] = "RS_Other"

# Condition 1
table(home$Condition1)[order(table(home$Condition1))]
levels(home$Condition1) <- c(levels(home$Condition1), "C_Other") 
home$Condition1[home$Condition1 %in% c("RRNe","RRNn","PosA","RRAe","PosN","RRAn","Artery")] = "C_Other"

# ExterCond
table(home$ExterCond)[order(table(home$ExterCond))]
levels(home$ExterCond) <- c(levels(home$ExterCond), "EC_Other") 
home$ExterCond[home$ExterCond %in% c("Po", "Ex", "Fa", "Gd")] = "EC_Other"

# OverallQual
table(home$OverallQual)
levels(home$OverallQual) <- c(levels(home$OverallQual), "Very_Low") 
home$OverallQual[home$OverallQual %in% c("-4.27238846180047", "-3.28279659466729", "-2.38962301582022")] = "Very_Low"
```
- For some features, we were not sure wether or not they had an influence, so we tested the model with and without them and removed them if they were not improving our score : HeatingQC, SaleCondition, BsmtFinType2, RoofStyle, ExterQual. 

```{r, include = FALSE}
full_model = lm(log(SalePrice)~., data = home_streamlined)
summary(full_model)
```
Running a model with all the variables excluding the ones we just removed, we obtain an adjusted R2 of 0.91. 

According to the T test above, the variables having the most impact to explain SalePrice are the ones describing: 
- the location of the home (e.g. MSZoning, Neighborhood)
- the area of the home (LotArea)
- overall quality and condition ( OverallQual, OverallCond)
- # Comment/reword the specificities of some aspects of the home such as the roof (e.g. RoofStyleShed, RoofMat) and security functions (e.g. Fireplaces). 
- the construction period (e.g. YearBuilt, YearRemodAdd)

Many variables could be removed from our model while marginally affecting its efficiency to explain SalePrice. To improve model efficiency, we will perform a selection of variables based on forward, backward and both methods. 

```{r, include = FALSE}
# Reducing our number of features using the 3 selection methods seen in class (forward, backward, both)
library(MASS)
# Forward method :
linear_select_variables_forward = stepAIC(full_model, data=home_streamlined, trace=0, direction=c("forward"), verbose = FALSE)
summary(linear_select_variables_forward)

# Backward method
linear_select_variables_backward = stepAIC(full_model,~., trace=0, direction=c("backward") )
summary(linear_select_variables_backward)

# Both
linear_select_variables_both = stepAIC(full_model, trace=0, direction=c("both"))
summary(linear_select_variables_both)
```

To choose among the three methods, we retrieve the AIC of each model and choose the one with the smallest AIC.  
```{r}
extractAIC(linear_select_variables_forward)
extractAIC(linear_select_variables_backward)
extractAIC(linear_select_variables_both)
```
We choose the model extracted by the backward or the both method as they have the same AIC, smaller than the one of the forward method. We arbitrarily choose the backward model. For this specific model, let's verify that the postulates are verified.   
  
```{r}
# Validate (or not) the postulates 
par(mfrow=c(3,3))
plot(linear_select_variables_backward, which=1) # P1
plot(linear_select_variables_backward, which=3) # P2
acf(residuals(linear_select_variables_backward), main= "Auto-correlation plot") # P3
plot(linear_select_variables_backward, which=2) # P4
plot(linear_select_variables_backward, which=5) # Cook
```
Looking at the graphs, the postulates are verified for this model.  
Now, we want to verify that we don't have outliers in our model. 
```{r}
library(car)
influenceIndexPlot(linear_select_variables_backward)
```
Cook's distance plot: according to the Cook's criteria, we don't observe any leverage point or regression outlier.  
Studentized residuals plot: according to the plot, there are a lot of outliers (many points below -2), which is confusing. The two main outliers according to this criteria oare 524 and 1299. 
Bonferroni's plot : we notice that 13 points have a p-value inferior to 0.5, so they are outliers according to this criteria.  
Hat plot: two points (327 and 783) seem to be leverage points according to this criteria.  
  
Based on those results, we decided to run an outlier test for more precision: 
```{r}
# Finding the outliers of our model
outlierTest(linear_select_variables_backward)
```
Using this test, 8 possible outliers are given according to the studentized criterion and Bonferroni p-value. We can now test our model without them.
Note that those tests are based on confidence intervals and then for this reason, given the number of observations, we could have up to 70 outliers. 

  
```{r}
# Plotting our regression outliers
home_streamlined[c(199, 336, 596, 633),1:ncol(home_streamlined)]
```

```{r}
# Building the dataframe without those outliers 
home_no_outliers = home_streamlined[-c(199, 336, 596, 633), ]
```

```{r}
# Displaying our regression model to use it on our new dataframe without outliers
linear_select_variables_backward
```

```{r}
# Building the same regression model as before with our new dataframe 
linear_backward_without_outliers <- lm(formula = log(SalePrice) ~ MSSubClass + MSZoning + LotFrontage + 
    LotArea + LotConfig + Neighborhood + Condition1 + OverallQual + 
    YearBuilt + YearRemodAdd + MasVnrType + MasVnrArea + BsmtCond + 
    BsmtExposure + BsmtFinSF1 + BsmtUnfSF + TotalBsmtSF + CentralAir + 
    `1stFlrSF` + GrLivArea + BsmtFullBath + FullBath + HalfBath + 
    KitchenQual + Fireplaces + GarageType + GarageYrBlt + GarageCars + 
    GarageQual + WoodDeckSF + MoSold + YrSold, data = home_streamlined)
```

```{r}
# Looking at the results
#summary(linear_backward_without_outliers)
```

```{r, message= FALSE, comment=FALSE}
# Testing the postulates on our new model 
par(mfrow=c(3,3))
plot(linear_backward_without_outliers, which=1) # P1
plot(linear_backward_without_outliers, which=3) # P2
acf(residuals(linear_backward_without_outliers), main= "Auto-correlation plot") # P3
plot(linear_backward_without_outliers, which=2) # P4
plot(linear_backward_without_outliers, which=5) # outliers wusing cook's distance
```
According to the graphs, P1 and P3 are validated without hesitation. 
For P2, we notice an slight elliptic behaviour in the middle, but it seems slight enough to validate the postulate. 
For P4, we have tail observations that do not fit the normal distribution. Based on the high number of observations in our dataframe and the few number of points not aligned with the normal distribution quantiles, we validate the postulate. 

\textbf{IV. Final models}  
  
Parameters of our model:  
Error:  

```{r, include=FALSE}
test = test[,2:ncol(test)]
# Reaffecting test's features to factors
test$OverallQual = as.factor(test$OverallQual)
test$OverallCond = as.factor(test$OverallCond)
test$MoSold = as.factor(test$MoSold)
test$MSSubClass = as.factor(test$MSSubClass)
test$Fireplaces = as.factor(test$Fireplaces)

# Regrouping neighbourhoods of less than 20 and 50 sales together
levels(test$Neighborhood) <- c(levels(test$Neighborhood), "N_Under20Sales","N_Under50Sales") 
test$Neighborhood[test$Neighborhood %in% c("Blueste","NPkVill","Veenker","BrDale","Blmngtn","MeadowV")] = "N_Under20Sales"
test$Neighborhood[test$Neighborhood %in% c("StoneBr","SWISU","ClearCr","IDOTRR","Timber","NoRidge","Mitchel")] = "N_Under50Sales"

# Roofstyle
levels(test$RoofStyle) <- c(levels(test$RoofStyle), "RS_Other") 
test$RoofStyle[test$RoofStyle %in% c("Shed","Mansard","Gambrel","Flat")] = "RS_Other"

# Condition 1
levels(test$Condition1) <- c(levels(test$Condition1), "C_Other") 
test$Condition1[test$Condition1 %in% c("RRNe","RRNn","PosA","RRAe","PosN","RRAn","Artery")] = "C_Other"

# ExterCond
levels(test$ExterCond) <- c(levels(test$ExterCond), "EC_Other") 
test$ExterCond[test$ExterCond %in% c("Po", "Ex", "Fa", "Gd")] = "EC_Other"

# OverallQual
levels(test$OverallQual) <- c(levels(test$OverallQual), "Very_Low") 
test$OverallQual[test$OverallQual %in% c("-4.27238846180047", "-3.28279659466729", "-2.38962301582022")] = "Very_Low"
```


```{r}
# Test error
sqrt(mean(test$SalePrice-exp(predict.lm(linear_backward_without_outliers, test)))^2)
```
```{r}
sqrt(mean(log(test$SalePrice)-predict.lm(linear_backward_without_outliers, test))^2)
```

Confidence interval:
```{r}
newgrille = data.frame(LotArea=seq(min(home_no_outliers$LotArea)+1,max(home_no_outliers$LotArea)-1),2) 
predicgrille = predict.lm(linear_backward_without_outliers, newgrille, interval='confidence',level=0.95) 
plot(home_no_outliers$LotArea,home_no_outliers$SalePrice,ylab="SalePrice",xlab="LotArea",pch=20)
#abline(linear_backward_without_outliers,col='red')
matlines(newgrille$SalePrice,predicgrille,lty=c(1,2,2),col=c("red","blue","blue")) 
predicgrille = predict.lm(linear_backward_without_outliers,newgrille, interval='prediction',level=0.95) 
matlines(newgrille$SalePrice,predicgrille,lty=c(1,2,2),col=c("red","green","green"))
```
```{r}
# Assuming gaussianity 
summary(linear_backward_without_outliers)$coefficients
```



```{r}
p-value:  
Estimate of the generalization error of our final model:  training data 
  
\textbf{V. Discussion}  
Final conclusions on our model:  
key learnings:  
Improvements for future projects: