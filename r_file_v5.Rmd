---
title: "Regression - Final project"
author: "Victoire de Termont and Sarah Jallot"
date: "12/3/2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
```
Our objective is to predict the prices of residential homes in Ames, Iowa accurately by exploiting an existing dataset to its best. Our key metric will be the MSE on test data.     
The training data consists of 66 categorical and quantitative variables extensively describing 1,095 residential homes in Ames, Iowa, and the houses' corresponding price in dollars. Discriminating between relevant and non relevant regressors to foster sparsity in our model is paramount for an efficient and robust model.  
    
To achieve this, we first investigate numerical and factor variables sequentially. Applying the log to SalePrice likens it to a Gaussian distribution, with a few outliers. A PCA analysis on numerical variables showed us that 19/23 numerical variables accounted for 99% of the sample variance. Factorwise, we distinguish three types of regressors: factors with an overrepresented level, some with high cardinality, and others with standard level repartition. 
  
We then perform elementary factor pruning and intra-factor modality regrouping before using ANOVA to remove factors manually. We do not touch numerical variables but instead rely on our model to make a selection.  
  
We choose to fit two linear models using stepwise selection procedure with the AIC criterion to foster sparsity. In the first model, the only modification we impose on the output is applying the logarithm, before removing outliers after testing them. In the second model, we apply a winsorisation method to log(SalePrice) as we noticed that most outliers located to the LHS of price made our model less robust.  In both cases, our best model according to AIC is the backward selection model. Model 1 predicts the test data much more accurately than model 2 (24 000$ vs 33 000$), but model 2 validates the regression assumptions better. XXXX what do we predict better?? We choose to retain model 1 as we want our model to generalise well, and model 1 is satisfactory enough on the postulates. 
Our final model has an MSE of XX.  
    
  
\textbf{I. Introduction - Data description} 
We first explore the pre-processed data to get intuition of an efficient model. Understanding and preprocessing the data implies that we understand what we are given, check that there are no missing values, and that R correctly categorises each regressor. 

```{r, include = FALSE}
load("DataProject.RData")
home = train[,2:ncol(train)]
head(home)
```
The data we are handling is heterogeneous, although it is presented in a structured manner. We are dealing both with categorical and quantitative variables, which we will consider separately.  

`````{r, include = FALSE } 
summary(home)
```
We observe in the data summary that a full model would include 66 features and an intercept, and 1,095 observations.  
The list of features is extensive: not all regressors will be useful to predict house price. For instance, we expect the variable MSZoning to have much more impact on the price that the variable Heating, as the heating system is something that can be changed, whereas the location is permanent.  
Quantitative variables differ in scale and range: prices start from ~35,000 dollars, and can attain 755,000. Before pre-processing, surfaces took higher values than bedrooms above ground which ranged from 1 to 8. Scaling the data allows to harmonise it, so we keep the scaling. 
    
```{r, include = FALSE}
str(home, give.attr = FALSE)
```
R appears to cast some factors as integers: mainly areas and ratings. We decide not to consider years as factors as we want our predictions to generalise to other, unencountered years. We recast OverallQual, OverallCond, MoSold, MSSubClass and Fireplaces as factors.  
All quantitative features are integers and price, the output, is the only numeric. We know that R automatically encodes factors, and we choose to keep the by default levels, which are  alphabetically ordered. 

```{r, include = FALSE}
home$OverallQual = as.factor(home$OverallQual)
home$OverallCond = as.factor(home$OverallCond)
home$MoSold = as.factor(home$MoSold)
home$MSSubClass = as.factor(home$MSSubClass)
home$Fireplaces = as.factor(home$Fireplaces)
```
Recasting some of our features as factors improves our predictions but decreases model robustness, so we will have to select the factors we keep carefully. 
  
We check there are no missing values in the pre-processed data before launching into the analysis.  
```{r}
cat("There are", sum(is.na(home)), "missing values in our dataframe.")
```
  
\textbf{II. Exploratory data analysis - Initial modeling} 
  
1. Numerical analysis.  
We describe the output data before analysing the correlation matrix. A PCA analysis we performed showed us that 23/28 numerical variables accounted for 99% of the variance. We left it out for concision. 
```{r, include = FALSE}
# Extracting all numerical variables
nums = unlist(lapply(home, is.numeric))
home_numerical = (home[ , nums])
```
We first set out to observe the output, SalePrice. 
```{r, fig.width = 5, fig.height = 4, fig.align = "center"}
par(mfrow=c(1,2))
# A few observations on SalePrice. 
hist(home_numerical$SalePrice, main = "SalePrice", cex.main = 0.8, cex.axis = 0.5, cex.lab = 0.5)
boxplot(home_numerical$SalePrice, main = "SalePrice", cex.main = 0.8, cex.axis = 0.5, cex.lab = 0.5)
```

```{r}
library(e1071) 
cat("Data skewness is", skewness(home$SalePrice))
```
SalePrice is highly skewed to the right : we confirm this by noting that Saleprice mean is ~181,196 whereas the median is much lower at ~164,500. 
SalePrice is volatile with many houses to the left hand side, but with a number of outliers to the right with extreme values. To smoothen the output and approach a normal distribution, we will consider the log when fitting our model. If this isn't enough, we could go a step further by either trimming or modifying outlier values to improve our generalisation error.

```{r, fig.width = 5, fig.height = 3, fig.align = "center"}
par(mfrow=c(1,3))
hist(log(home$SalePrice), cex.main = 1.2, main = "log(SalePrice)")
boxplot(log(home_numerical[,"SalePrice"]),cex.main = 1.2)
qqnorm(log(home_numerical$SalePrice),cex.main = 1.2)
```
log(SalePrice) is pretty close to a normal distribution, except for extreme values. 

```{r, fig.width = 3, fig.height = 3, fig.align = "center"}
# Correlation matrix for numerical features
library(corrplot)
R = round(cor(home_numerical),2)
corrplot(R, method='ellipse', tl.cex = 0.5)
```
The numerical features that are the most correlated with SalePrice: GrLivArea, GarageArea and GarageCars, 1st & 2ndFlrSF, YearBuilt GarageYrBuilt & YearRemodAdd. Areas and surfaces are all related to square feet, which we know is a key driver in house sales. The construction or modernisation works are an indicator of overall quality of the housing and the investments that went into it, so it makes sense for them to be correlated to SalePrice. 
On the contrary, YrSold and BsmtHalfBath are poorly correlated to SalePrice. YrSold is correlated to none of the other features, so it is an irrelevant regressor: the decision to sell a house doesn't have much to do with what drives its price or the price one can sell it at. 
The correlation matrix does not take into account feature interactions, so we will leave numerical feature selection to our stepwise model procedures. 

Our intuition is that three feature types mainly drive SalePrices: area/surface, location and quality. 
Let's describe 1stFlrSF as it is the closest feature to square feet with 2ndFloorSF.
```{r, fig.width = 6, fig.height = 4, fig.align = "center"}
par(mfrow=c(1,3))
# Surface
hist((home_numerical$`1stFlrSF`), main = "1stFloorSF", cex.main = 0.8)
boxplot((home_numerical$`1stFlrSF`), main = "1stFloorSF", cex.main = 0.8)
qqnorm(home_numerical$`1stFlrSF`,cex.main = 1.2)
```
From these three graphs, we can assume a Gaussian distribution on 1stFloorSF. 
  

Because PCA works best with (scaled) numerical data, we perform it on the numerical features of our preprocessed data. 19/23 numerical variables account for ~99% of the variance. 
```{r, fig.width = 5, fig.height = 3, fig.align = "center", include = FALSE}
home_numerical_output = home_numerical$SalePrice
home_numerical_features = home_numerical[,2:ncol(home_numerical)] # we will perform the PCA analysis on this dataset. 

pca.train = home_numerical_features
prin_comp = prcomp(pca.train, center = TRUE, scale. = TRUE)

# Compute standard deviation of each principal component
std_dev = prin_comp$sdev

# Compute variance
pr_var = std_dev^2
prop_varex = pr_var/sum(pr_var)

# Cumulative scree plot
plot(cumsum(prop_varex), 
     xlab = "Principal Component", 
     ylab = "Cumulative Proportion of Variance Explained",
     type = "b")

abline(h=1, col='red')
abline(v=19, col='black', lty = 3)
```

3. Factor analysis  
We first investigate level fragmentation within the factors. We discover three types of factors, as per examples below.  
```{r, fig.width = 5, fig.height = 3, fig.align = "center"}
par(mfrow=c(1,3))
# Getting our factors
factors = unlist(lapply(home, is.factor))
home_factors = home[,factors]

# Street 
barplot(table(home_factors$Street)[order(table(home_factors$Street))], main = "Street")
# Neighbourhood
barplot(table(home_factors$Neighborhood)[order(table(home_factors$Neighborhood))], main = "Neighbourhood")
# Garage Finish
barplot(table(home_factors$GarageFinish)[order(table(home_factors$GarageFinish))], main = "GarageFinish")
```
i) Clear underrepresentation of some levels. 
In Street and Utilities, which are binary, this is conspicuous. From this we infer that these factors will not be useful in predicting house price in general: nearly all houses will be in the same category along these factors (and for those who are not, data is too sparse to generalise well).  
Note that this is also the case for RoofMatl, Heating, BsmtFinType2, Electrical, GarageCond, GarageQual.  
  
ii) High cardinality in the number of levels: this is especially the case for neighbourhood, which has 25 levels. We regroup some of these levels together to improve our predictions.
Note that this is also the case for Exterior1st, Exterior2nd, BsmtExposure. 
  
iii) Classic factor repartition with reasonable representation of each modality, as is the case for Housestyle for instance. 
Note that this is also the case for HeatingQC, GarageFinish, BsmtFinType1.  

Intuitively, we said that both location and overall quality will impact SalePrice significantly. Let us check this with anova and ancova tests. 

```{r, fig.height = 3, fig.align = "center"}
library(gridExtra)
# Area
par(mfrow=c(1,2))
library(ggplot2)
plot1 = ggplot(home, 
       aes(x=MSZoning, 
           y=SalePrice, 
           colour=MSZoning, 
           group = MSZoning, 
           fill = MSZoning)) + geom_boxplot(alpha=0.5, outlier.alpha=0) +geom_jitter(width=0.1)

# Overall quality 
plot2 = ggplot(home, 
       aes(x=OverallQual, 
           y=SalePrice, 
           colour= OverallQual, 
           group = OverallQual, 
           fill = OverallQual)) + geom_boxplot(alpha=0.5, outlier.alpha=0) +geom_jitter(width=0.1)

grid.arrange(plot1,plot2,ncol=2)
```
It appears that houses from the RM and RH areas are less expensive than the ones from FV and RL areas.
```{r}
# Anova on area
mod1 = lm(SalePrice ~ MSZoning-1 , data=home)
anova(mod1)

# Anova on OverallQual
mod2 = lm(SalePrice ~ OverallQual-1 , data=home)
anova(mod2)
```
Both area and overall quality have a strong effect on SalePrice. Let's perform an ancova to evaluate interaction importance. 
```{r}
# ANCOVA on quality and area
mod3 = lm(SalePrice~1 + MSZoning + OverallQual + OverallQual:MSZoning, data = home)
anova(mod3)
```
SalePrice highly depends on MSZoning, OverallQual and their interaction. So we will keep these factors.  

\textbf{III. Modeling and diagnostics}  
Given the number of regressors, we choose models favouring feature sparsity. We implement stepwise selection with an AIC criterion. 
We predict SalePrice's logarithm to improve our target smoothness and estimated residuals homoscedasticity. In our first model, we notice that our 8 outliers are located towards the extreme bottom values of SalePrice. They prevent us from validating our postulates, mainly homoscedasticity and gaussianity. Removing them gives a more robust model, but we thought of testing a second approach. We fit a second model where instead of removing the outliers after model fit, we use the winsor method to reaffect extreme values and normalise our data before model fit. Doing this improves model robustness (the postulates) but also meaningfully increases our MSE on test data. We choose to keep model1 for our fit as the aim is to predict data accurately.  
  
```{r, include = FALSE}
winsor1 <- function (x, fraction=.05)
{if(length(fraction) != 1 || fraction < 0 ||fraction > 0.5) {stop("bad value for 'fraction'")}
lim <- quantile(x, probs=c(fraction, 1-fraction))
x[ x < lim[1] ] <- lim[1]
x[ x > lim[2] ] <- lim[2]
x}
```
We noted earlier that many categorical variables could be considered uninformative or redundant. 
- We removed factors with massively overrepresented categories: Street, Utilities, RoofMatl, Condition2, Heating, Electrical, Functional, GarageCond.  
We also made some regroupments within factors to diminish cardinality, mostly for neighbourhood. 
- Based on Anova tests, we removed other factors to improve model robustness: OverallCond, Exterior1st, Exterior2nd. OverallCond in particular was too specific and weakened our model by creating observations with leverage one.
```{r}
# Ancova on OverallQual and OverallCond
mod5 <- lm(SalePrice~1 + OverallQual + OverallCond + OverallQual:OverallCond, data = home)
anova(mod5)
```
- For some features, we were not sure wether or not they had an influence, so we tested the model with and without them and removed them if they were not improving our score : HeatingQC, SaleCondition, BsmtFinType2, RoofStyle, ExterQual. 
```{r, include = FALSE}
col_to_remove <- c("Street", "Utilities", "RoofMatl", "Condition2","Heating", "Electrical", "Functional", "GarageCond","Exterior1st", "Exterior2nd", "OverallCond", "HeatingQC", "SaleCondition", "BsmtFinType2","RoofStyle", "ExterQual")
for (name in col_to_remove){cat(which(colnames(home)==name)," ")}
```

```{r, include=FALSE}
# Elementary pruning and regrouping of categories
# Regrouping neighbourhoods of less than 20 and 50 sales together 
table_neighborhood <- table(home$Neighborhood)
table_neighborhood[order(table_neighborhood)]

levels(home$Neighborhood) <- c(levels(home$Neighborhood), "N_Under20Sales","N_Under50Sales") 
home$Neighborhood[home$Neighborhood %in% c("Blueste","NPkVill","Veenker","BrDale","Blmngtn","MeadowV")] = "N_Under20Sales"
home$Neighborhood[home$Neighborhood %in% c("StoneBr","SWISU","ClearCr","IDOTRR","Timber","NoRidge","Mitchel")] = "N_Under50Sales"

# Roofstyle
table(home$RoofStyle)[order(table(home$RoofStyle))]
levels(home$RoofStyle) <- c(levels(home$RoofStyle), "RS_Other") 
home$RoofStyle[home$RoofStyle %in% c("Shed","Mansard","Gambrel","Flat")] = "RS_Other"

# Condition 1
table(home$Condition1)[order(table(home$Condition1))]
levels(home$Condition1) <- c(levels(home$Condition1), "C_Other") 
home$Condition1[home$Condition1 %in% c("RRNe","RRNn","PosA","RRAe","PosN","RRAn","Artery")] = "C_Other"

# ExterCond
table(home$ExterCond)[order(table(home$ExterCond))]
levels(home$ExterCond) <- c(levels(home$ExterCond), "EC_Other") 
home$ExterCond[home$ExterCond %in% c("Po", "Ex", "Fa", "Gd")] = "EC_Other"

# OverallQual
table(home$OverallQual)
levels(home$OverallQual) <- c(levels(home$OverallQual), "Very_Low") 
home$OverallQual[home$OverallQual %in% c("-4.27238846180047", "-3.28279659466729", "-2.38962301582022")] = "Very_Low"
```

```{r, include=FALSE}
home_streamlined <- home[-c(5, 8, 21, 13, 37, 40, 51, 59, 22, 23, 17, 38, 66, 34, 20, 26, 17)]
```
  
After working on our factors, we start by fitting a full linear model to predict log(SalePrice).
```{r, include = FALSE}
full_model1 <- lm(log(SalePrice)~., data = home_streamlined)
summary(full_model1)
```
Running a model with all the variables excluding the ones we just removed, we obtain an adjusted R-squared of 0.91. 
Many variables could be removed from our model while marginally affecting its efficiency to explain SalePrice. To improve model efficiency, we perform a selection of variables based on forward, backward and both methods. 
```{r, include = FALSE}
library(MASS)
# Forward method, backward method, and both to reduce our number of features
linear_select_variables_forward = stepAIC(full_model1, data=home_streamlined, trace=0, direction=c("forward"), verbose = FALSE)
linear_select_variables_backward = stepAIC(full_model1,~., trace=0, direction=c("backward") )
linear_select_variables_both = stepAIC(full_model1, trace=0, direction=c("both"))
```
To choose among the three methods, we retrieve the AIC of each model and choose the one with the smallest AIC.  
```{r}
extractAIC(linear_select_variables_forward)
extractAIC(linear_select_variables_backward)
extractAIC(linear_select_variables_both)
```
The 33-feature model extracted by backward and both is the same. Its AIC is smaller than the one of the forward method. We arbitrarily choose the backward model. 
Let's verify that the postulates.   
```{r}
# Validate (or not) the postulates 
par(mfrow=c(3,3))
plot(linear_select_variables_backward, which=1) # P1
plot(linear_select_variables_backward, which=3) # P2
acf(residuals(linear_select_variables_backward), main= "Auto-correlation plot") # P3
plot(linear_select_variables_backward, which=2) # P4
plot(linear_select_variables_backward, which=5) # Cook's distance
```
We validate P1, centered errors, without hesitation.  
We notice a slight elliptical behaviour of our residuals in the Scale-Location plot, potentially problematic for P3 or residual homoscedasticity. Our model may predict extreme SalePrice values less well than others. However, we validate the postulate given the plot range of that behaviour (between 1 and 1.5). Observations 336, 199 and 596 have particularly high standardised residuals. 
We do not validate P3, the uncorrelation assumption - one bar exceeds the fitted-line threshold.
P4 can be validated, but again we notice the strange behaviour of tail values, towards the left-hand-side this time. Points which are not aligned with the normal distribution quantiles are limited given the number of observations we have. Note however that observations 199, 336 and 596 appear again: they are far from the theoretical quantiles. 
None of our observations has a Cook distance bigger than one. Observation 199 is the closest to this threshold, meaning it has significant leverage and residual abnormality.The second closest is observation 596, again. 
 
Let's perform an outlier analysis to see if this helps to validate our postulates. 
```{r, fig.width = 4, fig.height = 3, fig.align = "center"}
library(car)
influenceIndexPlot(linear_select_variables_backward)
```
Unsurprisingly, observations 199 and 596 are very clear outliers in nearly all our tests. In the Cook's distance plot, they are close to the 1 threshold albeit under it. Their studentised residuals values are smaller than -8 (versus a -2 assumption for a 95% confidence interval), Bonferroni is of order e-13, the hat values of order e-16. 
Studentized residuals plot: 30 points are below -2, and 23 are over 2. Our model allows for ~55 outliers within a 95% confidence interval and we are within that threshold. Extreme values are preoccupying: observations 199, 336, 596, 618, 633, 692 and 809 are smaller than -4. 
Bonferroni's plot : 6 points have a p-value inferior to 0.5, so they are outliers according to this criterion.  
Hat plot: observations 459 and 687 have leverage at 0.68, well over 0.5.  

Based on those results, we decided to run an outlier test for more precision: 
```{r}
# Finding the outliers of our model
outlierTest(linear_select_variables_backward)
```
We confirm that 199, 596 and 336 are very clear outliers (and they have significant leverage). So we remove them without question. The other outliers we get by the outlier test are outliers by far also, so we remove them. 
```{r}
# Plotting our regression outliers
home_streamlined[c(199, 596, 336, 692, 618, 633, 323, 809),1:ncol(home_streamlined)] 
```

```{r, include = FALSE}
# Building the dataframe without those outliers 
home_no_outliers = home_streamlined[-c(199, 596, 336, 692, 618, 633, 323, 809)]
```

```{r, include = FALSE}
# Getting our regression model to use it on our new dataframe without outliers
summary(linear_select_variables_backward)
```

```{r}
# The first model we will examine is the following:
model1 = lm(formula = log(SalePrice) ~ MSSubClass + MSZoning + LotFrontage + 
    LotArea + LandContour + LotConfig + LandSlope + Neighborhood + 
    Condition1 + OverallQual + YearBuilt + YearRemodAdd + MasVnrType + 
    MasVnrArea + ExterCond + Foundation + BsmtCond + BsmtExposure + 
    BsmtFinSF1 + BsmtUnfSF + CentralAir + GrLivArea + BsmtFullBath + 
    FullBath + HalfBath + KitchenQual + Fireplaces + GarageType + 
    GarageCars + GarageQual + WoodDeckSF + MoSold + YrSold, data = home_no_outliers)
```

```{r, include = FALSE}
summary(model1)
```
We now have an adjusted r-squared of 93% on train data.  
  
```{r, message= FALSE, comment=FALSE}
# Testing the postulates on our model 1
par(mfrow=c(3,3))
plot(model1, which=1) # P1
plot(model1, which=3) # P2
acf(residuals(model1), main= "Auto-correlation plot") # P3
plot(model1, which=2) # P4
plot(model1, which=5) # Cook's distance
```
The postulates are about verified for this model. 
Residuals are centered. Scale-location is stil slightly elliptic, but we accept it. We now validate autocorrelation.Gaussianity still poses problem for tail values. And no observation has significant Cook distance. 
```{r, include = FALSE}
home_streamlined$SalePrice= (winsor1(home$SalePrice, fraction=.05))
full_model2 = lm(log(SalePrice)~., data = home_streamlined)
linear_select_variables_backward_winsor = stepAIC(full_model2,~., trace=0, direction=c("backward") )
```

```{r, include = FALSE}
summary(linear_select_variables_backward_winsor)
```
We have an adjusted r-squared of 91% on train data. 
```{r}
par(mfrow=c(3,3))
plot(linear_select_variables_backward_winsor, which=1) # P1
plot(linear_select_variables_backward_winsor, which=3) # P2
acf(residuals(linear_select_variables_backward_winsor), main= "Auto-correlation plot") # P3
plot(linear_select_variables_backward_winsor, which=2) # P4
plot(linear_select_variables_backward_winsor, which=5) # Cook's distance
```
This time, residuals are perfectly centered. We validate homoscedasticity and uncorrelation without question. Gaussianity is still troublesome for tail values. No observation has significant Cook's distance. Again, observations 336, 199, 596 can be singled out on all plots. 
```{r, fig.width = 4, fig.height = 3, fig.align = "center"}
influenceIndexPlot(linear_select_variables_backward)
```

```{r, include = FALSE}
outlierTest(linear_select_variables_backward_winsor)
```
We perform the outlier test to confirm our outliers. Without surprise, 199, 336 and 596 are regression outliers. 633 also is. We remove them. 
```{r, include = FALSE}
home_streamlined[c(199, 336, 596, 633),1:ncol(home_streamlined)]
home_no_outliers = home_streamlined[-c(199, 336, 596, 633), ]
```

```{r}
model2 = lm(formula = log(SalePrice) ~ MSSubClass + MSZoning + LotFrontage + 
    LotArea + LotConfig + Neighborhood + Condition1 + OverallQual + 
    YearBuilt + YearRemodAdd + MasVnrType + MasVnrArea + BsmtCond + 
    BsmtExposure + BsmtFinSF1 + BsmtUnfSF + TotalBsmtSF + CentralAir + 
    `1stFlrSF` + GrLivArea + BsmtFullBath + FullBath + HalfBath + 
    KitchenQual + Fireplaces + GarageType + GarageYrBlt + GarageCars + 
    GarageQual + WoodDeckSF + MoSold + YrSold, data = home_no_outliers)
```

```{r, include = FALSE}
summary(model2)
```
We have a 93% R-squared on train data.
```{r fig.width = 3, fig.height = 2, fig.align = "center"}
plot(model2, which=2)
```
We approximately validate Gaussianity, while noting that it is the only assumption that is not always necessary among the postulates.
  
\textbf{IV. Final models}  
```{r, include=FALSE}
# Testing our model on the test set 
test = test[,2:ncol(test)]
# Reaffecting test's features to factors
test$OverallQual = as.factor(test$OverallQual)
test$OverallCond = as.factor(test$OverallCond)
test$MoSold = as.factor(test$MoSold)
test$MSSubClass = as.factor(test$MSSubClass)
test$Fireplaces = as.factor(test$Fireplaces)

# Regrouping neighbourhoods of less than 20 and 50 sales together
levels(test$Neighborhood) <- c(levels(test$Neighborhood), "N_Under20Sales","N_Under50Sales") 
test$Neighborhood[test$Neighborhood %in% c("Blueste","NPkVill","Veenker","BrDale","Blmngtn","MeadowV")] = "N_Under20Sales"
test$Neighborhood[test$Neighborhood %in% c("StoneBr","SWISU","ClearCr","IDOTRR","Timber","NoRidge","Mitchel")] = "N_Under50Sales"

# Roofstyle
levels(test$RoofStyle) <- c(levels(test$RoofStyle), "RS_Other") 
test$RoofStyle[test$RoofStyle %in% c("Shed","Mansard","Gambrel","Flat")] = "RS_Other"

# Condition 1
levels(test$Condition1) <- c(levels(test$Condition1), "C_Other") 
test$Condition1[test$Condition1 %in% c("RRNe","RRNn","PosA","RRAe","PosN","RRAn","Artery")] = "C_Other"

# ExterCond
levels(test$ExterCond) <- c(levels(test$ExterCond), "EC_Other") 
test$ExterCond[test$ExterCond %in% c("Po", "Ex", "Fa", "Gd")] = "EC_Other"

# OverallQual
levels(test$OverallQual) <- c(levels(test$OverallQual), "Very_Low") 
test$OverallQual[test$OverallQual %in% c("-4.27238846180047", "-3.28279659466729", "-2.38962301582022")] = "Very_Low"
```

```{r}
# Computing the test RMSE for model 1 and model 2 
cat("RMSE for model 1 is", sqrt(mean((test$SalePrice - exp(predict.lm(model1, test))) ^ 2)), " dollars")
cat(" and RMSE for model 2 is", sqrt(mean((test$SalePrice - exp(predict.lm(model2, test))) ^ 2)), " dollars")
```
Even though our first model is less robust than the second, its test error is much lower. So this is the one we will retain since it still validates our postulates. 
Let us analyse its coefficients in detail. 

Model1 can be written lm(formula = log(SalePrice) ~ MSSubClass + MSZoning + LotFrontage + 
    LotArea + LandContour + LotConfig + LandSlope + Neighborhood + 
    Condition1 + OverallQual + YearBuilt + YearRemodAdd + MasVnrType + 
    MasVnrArea + ExterCond + Foundation + BsmtCond + BsmtExposure + 
    BsmtFinSF1 + BsmtUnfSF + CentralAir + GrLivArea + BsmtFullBath + 
    FullBath + HalfBath + KitchenQual + Fireplaces + GarageType + 
    GarageCars + GarageQual + WoodDeckSF + MoSold + YrSold, data = home_no_outliers)
With a 92% adjusted r-squared on train, validation of our postulates and a 24.5k$ RMSE on test, it is the model that we keep. 
Its most important features are: 
-Location and area (MSZoning, neighbourhood)
-Quality and condition (OverallQual,GarageQual, KitchenQual)
-YearBuilt and YearRemodAdd
As expected, location , quality and condition, and construction/improvement year are our most important features with a high level of significance. 

Model 2 can be written: lm(formula = log(SalePrice) ~ MSSubClass + MSZoning + LotFrontage + 
    LotArea + LotConfig + Neighborhood + Condition1 + OverallQual + 
    YearBuilt + YearRemodAdd + MasVnrType + MasVnrArea + BsmtCond + 
    BsmtExposure + BsmtFinSF1 + BsmtUnfSF + TotalBsmtSF + CentralAir + 
    `1stFlrSF` + GrLivArea + BsmtFullBath + FullBath + HalfBath + 
    KitchenQual + Fireplaces + GarageType + GarageYrBlt + GarageCars + 
    GarageQual + WoodDeckSF + MoSold + YrSold, data = home_no_outliers)
It has a 93% adjusted R-squared on train, validates the postulates, and a 31.0K$ RMSE on test.
Again we see location with neighbourhood and quality (OverallQual, GarageQual) as important predictors. 

In both cases, testing our model against the intercept we obtain a p-value of order e-16: we confidently reject the hypothesis that the intercept-reduced model is better to explain SalePrice than our model.  
Confidence intervals: as our regression model is mostly impacted by factors, doing confidence intervals does not help us visualize the proximity among SalePrice and the most important features. However, doing a 95% confidence interval with LotArea, we notice that this variable is not good at explaning SalePrice by itself, as the price can double for a same value of LotArea. 
```{r, fig.width=5, fig.height=3, fig.align="center"}
model <- lm(SalePrice ~ LotArea, data=home)
plot(home$LotArea, home$SalePrice, xlab="LotArea", ylab="SalePrice", main="Regression", cex.main=0.8, cex.axis=0.5, cex.lab=0.5)
abline(model, col="lightblue")

newx <- seq(0, 7, by=0.05)
pred_interval <- predict(model, newdata=data.frame(LotArea=newx), interval="prediction",
                         level = 0.95)
lines(newx, pred_interval[,2], col="orange", lty=2)
lines(newx, pred_interval[,3], col="orange", lty=2)
```


```{r, fig.width=5, fig.height=3, include = FALSE}
library(glmnet)
#x_train = home[,1:]
#y_train
#fit = glmnet()
```

\textbf{V. Discussion}  
Compared to the full linear model run on the-processed dataset, our postulates are now much better validated with twice as less features (33 instead of 67).  
Plotting our errors as a function of SalePrice shows that we perform well under 300K dollars, and badly above with very high residuals for extreme values, which skew the mean residuals towards the top: 
```{r, fig.width=5, fig.height=3,fig.align = "center", }
# Graphical visualisation of our prediction errors
test_residuals = (test$SalePrice - exp(predict.lm(model2, test))) ^ 2
plot(test$SalePrice, test_residuals, main = "Test error as a function of SalePrice")
```
Our residuals are small for prices under 300K dollars, but increase a lot above. It would be interesting to fit a model on SalePrices that are above this threshold, another one on those under, and then average our predictions to make our final model. 
```{r}
cat("As we could infer from the plot, the skewness of our test residuals is very high at ", skewness(test_residuals))
```

Given that we still use 33 regressors, averaging the predictions of a lasso, a ridge and an elastic net could have been another solution to improve predictions and model robustness. We tried to do so but had struggles plotting our postulates afterwards, and therefore could not conclude. However, we will learn how to do it as it could have been very useful.  

\textbf{VI. Bonus}  
Here is a quick try at implementing the first solution: 

